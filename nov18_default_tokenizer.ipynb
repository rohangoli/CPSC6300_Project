{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "     |████████████████████████████████| 108 kB 2.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/rgoli/.local/lib/python3.7/site-packages (from tensorflow_hub) (1.21.4)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/rgoli/.local/lib/python3.7/site-packages (from tensorflow_hub) (3.19.1)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Dropout, LSTMCell, RNN, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import pickle\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import requests\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "import argparse\n",
    "from tokenize import tokenize, untokenize, COMMENT, STRING, NEWLINE, ENCODING, ENDMARKER, NL, INDENT, NUMBER\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Project_CodeNet_LangClass.tar.gz\"\n",
    "data_url = f\"https://dax-cdn.cdn.appdomain.cloud/dax-project-codenet/1.0.0/{file_name}\"\n",
    "\n",
    "# Download tar archive to local disk\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(requests.get(data_url).content)\n",
    "    \n",
    "# Extract contents of archive to local disk\n",
    "if os.path.exists(\"data\"):\n",
    "    shutil.rmtree(\"data\")    \n",
    "with tarfile.open(file_name) as tfile:\n",
    "    tfile.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lits = json.load(open(\"literals.json\"))\n",
    "\n",
    "def process_string(token, special_chars={\" \": \"U+0020\", \",\": \"U+002C\"}):\n",
    "    str_quote_options = [\"'''\", '\"\"\"', \"'\", '\"']\n",
    "    start_quote = \"\"\n",
    "    end_quote = \"\"\n",
    "    qualifier_regex = r\"^[a-z]+\"\n",
    "    qualifier_match = re.search(qualifier_regex, token)\n",
    "    # string qualifiers like 'r' for regex, 'f' for formatted string, 'b' for bytes, 'u' for unicode, etc (or combination of them)\n",
    "    qualifier = \"\" if not qualifier_match else qualifier_match[0]\n",
    "    # token string without qualifiers\n",
    "    token_string = re.sub(qualifier_regex, \"\", token)\n",
    "    # string literal without quotes\n",
    "    str_lit = token_string\n",
    "    for q in str_quote_options:\n",
    "        if token_string.startswith(q):\n",
    "            start_quote = q\n",
    "            str_lit = str_lit[len(q) :]\n",
    "            if token_string.endswith(q):\n",
    "                end_quote = q\n",
    "                str_lit = str_lit[: -len(q)]\n",
    "            break\n",
    "    # if start_quote in str_quote_options[:2]:\n",
    "    #     return \"\"\n",
    "    for sc in special_chars:\n",
    "        str_lit = str_lit.replace(sc, special_chars[sc])\n",
    "    return (\n",
    "        f\"{qualifier}{start_quote}<STR_LIT:{str_lit}>{end_quote}\"\n",
    "        if str_lit in lits['str']\n",
    "        else f\"{qualifier}{start_quote}<STR_LIT>{end_quote}\"\n",
    "    )\n",
    "\n",
    "def py_tokenize(file_type):\n",
    "    file_paths = glob.glob(os.path.join(os.getcwd(),\"data/\"+file_type+\"/Python\",\"*.*\"))\n",
    "    wf = open(os.path.join(os.getcwd(), f\"{file_type}.txt\"), 'w')\n",
    "    local_corpus = []\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            code = open(path).read()\n",
    "            token_gen = tokenize(BytesIO(bytes(code, \"utf8\")).readline)\n",
    "            out_tokens = []\n",
    "            prev_eol = False\n",
    "            for toknum, tokval, _, _, _ in token_gen:\n",
    "                tokval = \" \".join(tokval.split())\n",
    "                if toknum == STRING:\n",
    "                    add_token = process_string(tokval)\n",
    "                    out_tokens.append(add_token)\n",
    "                    prev_eol = False\n",
    "                elif toknum == NUMBER:\n",
    "                    if tokval in lits['num']:\n",
    "                        out_tokens.append(f\"<NUM_LIT:{tokval}>\")\n",
    "                    else:\n",
    "                        out_tokens.append(f\"<NUM_LIT>\")\n",
    "                    prev_eol = False\n",
    "                elif toknum in [NEWLINE, NL]:\n",
    "                    if not prev_eol:\n",
    "                        out_tokens.append(\"<EOL>\")\n",
    "                        prev_eol = True\n",
    "                elif toknum in [COMMENT, INDENT, ENCODING, ENDMARKER] or len(tokval) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    out_tokens.append(tokval)\n",
    "                    prev_eol = False\n",
    "            if out_tokens[0] == \"<EOL>\":\n",
    "                out_tokens = out_tokens[1:]\n",
    "            if out_tokens[-1] == \"<EOL>\":\n",
    "                out_tokens = out_tokens[:-1]\n",
    "        except Exception:\n",
    "            out_tokens = []\n",
    "#         local_corpus.extend((\" \".join(out_tokens)).split('<EOL>'))\n",
    "#         out_tokens = [\"<s>\"] + out_tokens + [\"</s>\"]\n",
    "        out = \" \".join(out_tokens)\n",
    "        local_corpus.append(out)\n",
    "        wf.write(out+\"\\n\")\n",
    "    print(f\"{file_type}: are done\")\n",
    "    wf.close()\n",
    "    return local_corpus\n",
    "\n",
    "def read_corpus(directory):\n",
    "    corpus = py_tokenize(directory)\n",
    "    full_corpus = ''.join(corpus)\n",
    "    corpus_new = []\n",
    "    for code in corpus:\n",
    "        corpus_new.extend(code.split('<EOL>'))\n",
    "        \n",
    "    return pd.DataFrame(corpus_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: are done\n",
      "test: are done\n"
     ]
    }
   ],
   "source": [
    "train_corpus = read_corpus(\"train\")\n",
    "train_corpus['target']=1\n",
    "test_corpus = read_corpus(\"test\")\n",
    "test_corpus['target']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>def perpendicular_bisector ( p , q ) :</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>x = ( q [ &lt;NUM_LIT:0&gt; ] - p [ &lt;NUM_LIT:0&gt; ] )</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>y = ( q [ &lt;NUM_LIT:1&gt; ] - p [ &lt;NUM_LIT:1&gt; ] )</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>return ( &lt;NUM_LIT:2&gt; * x , &lt;NUM_LIT:2&gt; * y , ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>def gauss_jordan_elimination ( Array ) :</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  target\n",
       "0            def perpendicular_bisector ( p , q ) :        1\n",
       "1     x = ( q [ <NUM_LIT:0> ] - p [ <NUM_LIT:0> ] )        1\n",
       "2     y = ( q [ <NUM_LIT:1> ] - p [ <NUM_LIT:1> ] )        1\n",
       "3   return ( <NUM_LIT:2> * x , <NUM_LIT:2> * y , ...       1\n",
       "4          def gauss_jordan_elimination ( Array ) :        1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>import sys</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MAX = &lt;NUM_LIT&gt;</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>L = [ &lt;NUM_LIT:2&gt; , &lt;NUM_LIT:3&gt; , &lt;NUM_LIT:5&gt;...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>def is_prime ( n ) :</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>if n == &lt;NUM_LIT:2&gt; : return True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  target\n",
       "0                                        import sys        1\n",
       "1                                   MAX = <NUM_LIT>        1\n",
       "2   L = [ <NUM_LIT:2> , <NUM_LIT:3> , <NUM_LIT:5>...       1\n",
       "3                              def is_prime ( n ) :        1\n",
       "4                 if n == <NUM_LIT:2> : return True        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\" \n",
    "model_name_code = \"microsoft/codebert-base\"\n",
    "# Download pytorch model\n",
    "model = AutoModel.from_pretrained(model_name_code)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(data):\n",
    "    encoded = tokenizer.batch_encode_plus(data,max_length = 25, pad_to_max_length=True,truncation=True, return_tensors='np')\n",
    "    return encoded.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x14f728972fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in train_corpus[0]]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "839          data_size1 = int ( len ( str ( data1 ) ) ) \n",
       "644                                     '''<STR_LIT>''' \n",
       "1334     return after_y1 , after_y2 , after_p , after_q \n",
       "2676                                        plist = [ ] \n",
       "3651        y1 , y2 = sorted ( f ( P . x ) for f in L ) \n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_corpus[0],train_corpus['target'], test_size=0.2, random_state=42)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenize_text(X_train.tolist())\n",
    "test_data = tokenize_text(X_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[    0,   414,  1215, ...,     1,     1,     1],\n",
       "        [    0,   128, 17809, ...,     1,     1,     1],\n",
       "        [    0,   671,    71, ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    0,   414,   134, ...,     1,     1,     1],\n",
       "        [    0,  1108,     2, ...,     1,     1,     1],\n",
       "        [    0,  1948, 32557, ...,  1437,     2,     1]]),\n",
       " 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 0]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': array([[    0, 46797,  5457, 49008,   179,   479,  1166,  1902,     2]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"input = stdin . readline\", return_tensors=\"np\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'input', 'Ġ=', 'Ġstd', 'in', 'Ġ.', 'Ġread', 'line', '</s>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([    0, 46797,  5457, 49008,   179,   479,  1166,  1902,     2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('nov18.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    net = tf.keras.layers.Dense(32, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.2)(net)\n",
    "    out = tf.keras.layers.Dense(5, activation='softmax')(net)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 27)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 27)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 27)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer_1 (KerasLayer)      [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 768)          0           keras_layer_1[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           49216       tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2080        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            165         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,533,702\n",
      "Trainable params: 109,533,701\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=27)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "# model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RobertaModel' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3cee7e23d823>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     validation_data=(\n\u001b[1;32m      5\u001b[0m         \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RobertaModel' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    np.array(y_train),\n",
    "    validation_data=(\n",
    "        test_data,\n",
    "        np.array(y_test)\n",
    "    ),\n",
    "    callbacks=[checkpoint, earlystopping],\n",
    "    batch_size=32,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fd003b217546f587b22bfe30bfcd15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/trainer_tf.py:114: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/master/examples/tensorflow\n",
      "  FutureWarning,\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/rgoli/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rohangoli/huggingface/runs/22evfqoz\" target=\"_blank\">./results</a></strong> to <a href=\"https://wandb.ai/rohangoli/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'cardinality'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-667ad1f5518b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/trainer_tf.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0mTrain\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mto\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \"\"\"\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_tfdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/trainer_tf.py\u001b[0m in \u001b[0;36mget_train_tfdataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_train_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train_examples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'cardinality'"
     ]
    }
   ],
   "source": [
    "from transformers import TFTrainingArguments, TFTrainer, TFRobertaForSequenceClassification\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "    model = TFRobertaForSequenceClassification.from_pretrained(model_name_code)\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_data,         # training dataset\n",
    "    eval_dataset=test_data             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# r_model = TFRobertForSequenceClassification(....)\n",
    "input_ids = tf.keras.layers.Input([None,], dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.layers.Input([None,], dtype=tf.int32, name=\"attention_mask\")\n",
    "token_type_ids = tf.keras.layers.Input([None,], dtype=tf.int32, name=\"token_type_ids\")\n",
    "output = model([input_ids, attention_mask, token_type_ids])\n",
    "model = tf.keras.models.Model(inputs=[input_ids, attention_mask, token_type_ids], output=output)\n",
    "\n",
    "model.compile(....)\n",
    "model.fit(....)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorflowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
