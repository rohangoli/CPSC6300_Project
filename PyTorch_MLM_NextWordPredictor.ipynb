{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) anaconda3/5.1.0-gcc/8.3.1     4) cudnn/8.0.0.180-11.0-linux-x64-gcc/7.5.0\n",
      "  2) anaconda3/2019.10-gcc/8.3.1   5) openjdk/1.8.0_222-b10-gcc/8.3.1\n",
      "  3) cuda/11.0.3-gcc/7.5.0         6) hadoop/3.2.1-gcc/8.3.1\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0+cu102'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print (torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/rgoli/.local/lib/python3.7/site-packages (4.12.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/rgoli/.local/lib/python3.7/site-packages (from transformers) (2021.11.2)\n",
      "Requirement already satisfied: sacremoses in /home/rgoli/.local/lib/python3.7/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/rgoli/.local/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/rgoli/.local/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: requests in /home/rgoli/.local/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rgoli/.local/lib/python3.7/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rgoli/.local/lib/python3.7/site-packages (from transformers) (21.2)\n",
      "Requirement already satisfied: filelock in /software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/rgoli/.local/lib/python3.7/site-packages (from transformers) (0.1.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/rgoli/.local/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: importlib-metadata in /home/rgoli/.local/lib/python3.7/site-packages (from transformers) (4.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/rgoli/.local/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/rgoli/.local/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/rgoli/.local/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/rgoli/.local/lib/python3.7/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rgoli/.local/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/rgoli/.local/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rgoli/.local/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: joblib in /software/spackages/linux-centos8-x86_64/gcc-8.3.1/anaconda3-2019.10-v5cuhr6keyz5ryxcwvv2jkzfj2gwrj4a/lib/python3.7/site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: six in /home/rgoli/.local/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in /home/rgoli/.local/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/codebert-base\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pytorch model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform input tokens \n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 31414,   232,   328,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model apply\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaTokenizer, RobertaForMaskedLM, pipeline\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained(\"microsoft/codebert-base-mlm\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base-mlm\")\n",
    "\n",
    "CODE = \"if (x is not None) <mask> (x>1)\"\n",
    "fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'if (x is not None) and(x>1)', 'score': 0.723699152469635, 'token': 8, 'token_str': ' and'}, {'sequence': 'if (x is not None) &(x>1)', 'score': 0.10633808374404907, 'token': 359, 'token_str': ' &'}, {'sequence': 'if (x is not None)and(x>1)', 'score': 0.021604200825095177, 'token': 463, 'token_str': 'and'}, {'sequence': 'if (x is not None) AND(x>1)', 'score': 0.021227475255727768, 'token': 4248, 'token_str': ' AND'}, {'sequence': 'if (x is not None) if(x>1)', 'score': 0.016991326585412025, 'token': 114, 'token_str': ' if'}]\n"
     ]
    }
   ],
   "source": [
    "outputs = fill_mask(CODE)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'for x in x', 'score': 0.2196567803621292, 'token': 3023, 'token_str': ' x'}, {'sequence': 'for x in self', 'score': 0.047300320118665695, 'token': 1403, 'token_str': ' self'}, {'sequence': 'for x in list', 'score': 0.03038906306028366, 'token': 889, 'token_str': ' list'}, {'sequence': 'for x in X', 'score': 0.01688920520246029, 'token': 1577, 'token_str': ' X'}, {'sequence': 'for x in y', 'score': 0.0151737155392766, 'token': 1423, 'token_str': ' y'}]\n"
     ]
    }
   ],
   "source": [
    "CODE = \"for x in <mask>\"\n",
    "outputs = fill_mask(CODE)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'for x in range ]', 'score': 0.13248734176158905, 'token': 27779, 'token_str': ' ]'}, {'sequence': 'for x in range()', 'score': 0.06554015725851059, 'token': 43048, 'token_str': '()'}, {'sequence': 'for x in range :', 'score': 0.06147528812289238, 'token': 4832, 'token_str': ' :'}, {'sequence': 'for x in range():', 'score': 0.0475279837846756, 'token': 49536, 'token_str': '():'}, {'sequence': 'for x in range )', 'score': 0.046191342175006866, 'token': 4839, 'token_str': ' )'}]\n"
     ]
    }
   ],
   "source": [
    "CODE = \"for x in range<mask>\"\n",
    "outputs = fill_mask(CODE)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 31414,   232,   328,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'input_ids': tensor([[    0, 31414,   232,   328,     2]]),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1]])},\n",
       " '_encodings': None,\n",
       " '_n_sequences': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'Hello', 'Ä world', '!', '</s>']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([    0, 31414,   232,   328,     2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorflowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
