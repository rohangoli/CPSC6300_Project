{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/how-to-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p code-tokenizer-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# paths = ['test_sent.txt', 'train_sent.txt']\n",
    "\n",
    "# # Initialize a tokenizer\n",
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# # Customize training\n",
    "# tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\",\n",
    "#     \"<pad>\",\n",
    "#     \"</s>\",\n",
    "#     \"<unk>\",\n",
    "#     \"<mask>\",\n",
    "#     \"<EOL>\",\"<NUM_LIT>\",\"<STR_LIT>\"\n",
    "# ])\n",
    "\n",
    "# # # Save files to disk\n",
    "# tokenizer.save_model(\".\", \"code-tokenizer-RoBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "# from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "# tokenizer = ByteLevelBPETokenizer(\n",
    "#     \"./code-tokenizer-RoBERTa-vocab.json\",\n",
    "#     \"./code-tokenizer-RoBERTa-merges.txt\",\n",
    "# )\n",
    "# tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#     (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#     (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "# )\n",
    "# tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "# print(\n",
    "#     tokenizer.encode(\"for i in range\")['input_ids']\n",
    "# )\n",
    "# # Encoding(num_tokens=7, ...)\n",
    "# # tokens: ['<s>', 'Mi', 'Ä estas', 'Ä Juli', 'en', '.', '</s>']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CodeDataset():\n",
    "#     def __init__(self, evaluate: bool=False):\n",
    "#         tokenizer = ByteLevelBPETokenizer(\n",
    "#             \"./code-tokenizer-RoBERTa-vocab.json\",\n",
    "#             \"./code-tokenizer-RoBERTa-merges.txt\",\n",
    "#         )\n",
    "#         tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#             (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#             (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "#         )\n",
    "#         tokenizer.enable_truncation(max_length=512)\n",
    "        \n",
    "#         self.example=[]\n",
    "        \n",
    "#         src_files=['test_sent.txt', 'train_sent.txt']\n",
    "        \n",
    "#         for src_file in src_files:\n",
    "#             print(\"ðŸ”¥\", src_file)\n",
    "#             lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "#             self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         return len(self.examples)\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         # Weâ€™ll pad at the batch level.\n",
    "#         return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['code-toknzr-roberta/vocab.json', 'code-toknzr-roberta/merges.txt']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from pathlib import Path\n",
    "\n",
    "paths = ['test_sent.txt', 'train_sent.txt']\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=8192, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\"<eol>\",\"<str_lit>\",\"<num_lit>\"\n",
    "])\n",
    "#Save the Tokenizer to disk\n",
    "!mkdir -p \"code-toknzr-roberta\"\n",
    "tokenizer.save_model(\"code-toknzr-roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'for', 'Ä i', 'Ä in', 'Ä range', '</s>']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the tokenizer using vocab.json and mrege.txt files\n",
    "tokenizer_folder = \"code-toknzr-roberta\"\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'vocab.json')),\n",
    "    os.path.abspath(os.path.join(tokenizer_folder,'merges.txt'))\n",
    ")\n",
    "# Prepare the tokenizer\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "# Test the tokenizer\n",
    "tokenizer.encode(\"for i in range\")\n",
    "# Show the tokens created\n",
    "tokenizer.encode(\"for i in range\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  49816064\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "# Set a configuration for our RoBERTa model\n",
    "config = RobertaConfig(\n",
    "    vocab_size=8192,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "# Initialize the model from a configuration without pretrained weights\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "print('Num parameters: ',model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code-toknzr-roberta/tokenizer_config.json',\n",
       " 'code-toknzr-roberta/special_tokens_map.json',\n",
       " 'code-toknzr-roberta/vocab.json',\n",
       " 'code-toknzr-roberta/merges.txt',\n",
       " 'code-toknzr-roberta/added_tokens.json',\n",
       " 'code-toknzr-roberta/tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "new_tokenizer = RobertaTokenizerFast(tokenizer_object=tokenizer)\n",
    "new_tokenizer.save_pretrained(\"code-toknzr-roberta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "# Create the tokenizer from a trained one\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"code-toknzr-roberta\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeDataset():\n",
    "    def __init__(self, datafile, tokenizer):\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "        self.examples = []\n",
    "        # For every value in the dataframe \n",
    "        with open(datafile,'r') as fp:\n",
    "            for example in fp:\n",
    "                # \n",
    "                x=tokenizer.encode_plus(example, max_length = 512, truncation=True, padding=True)\n",
    "                self.examples += [x.input_ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Weâ€™ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])\n",
    "      \n",
    "# Create the train and evaluation dataset\n",
    "train_dataset = CodeDataset(\"train_sent.txt\", tokenizer)\n",
    "eval_dataset = CodeDataset(\"test_sent.txt\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Use Multiple GPUs for Training\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0,1,2,3], dim=0)\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='dec4_gpt',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    save_steps=8192,\n",
    "    #eval_steps=4096,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Create the trainer for our model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    #prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3059992\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 17931\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrohangoli\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rohangoli/huggingface/runs/h13hkfek\" target=\"_blank\">dec4_gpt</a></strong> to <a href=\"https://wandb.ai/rohangoli/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='17931' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    6/17931 00:02 < 2:48:21, 1.77 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1097, in forward\n    return_dict=return_dict,\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 854, in forward\n    return_dict=return_dict,\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 529, in forward\n    output_attentions,\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 414, in forward\n    past_key_value=self_attn_past_key_value,\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 344, in forward\n    output_attentions,\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 267, in forward\n    attention_probs = self.dropout(attention_probs)\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/dropout.py\", line 58, in forward\n    return F.dropout(input, self.p, self.training, self.inplace)\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/functional.py\", line 1169, in dropout\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\nRuntimeError: CUDA out of memory. Tried to allocate 356.00 MiB (GPU 0; 15.78 GiB total capacity; 12.55 GiB already allocated; 41.19 MiB free; 14.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e5e6b3b9574e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1284\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1777\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1809\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py\", line 61, in _worker\n    output = module(*input, **kwargs)\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 1097, in forward\n    return_dict=return_dict,\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 854, in forward\n    return_dict=return_dict,\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 529, in forward\n    output_attentions,\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 414, in forward\n    past_key_value=self_attn_past_key_value,\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 344, in forward\n    output_attentions,\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py\", line 267, in forward\n    attention_probs = self.dropout(attention_probs)\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/modules/dropout.py\", line 58, in forward\n    return F.dropout(input, self.p, self.training, self.inplace)\n  File \"/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/functional.py\", line 1169, in dropout\n    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\nRuntimeError: CUDA out of memory. Tried to allocate 356.00 MiB (GPU 0; 15.78 GiB total capacity; 12.55 GiB already allocated; 41.19 MiB free; 14.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at saved_dec7_roberta_a were not used when initializing RobertaForMaskedLM: ['module.roberta.encoder.layer.0.attention.self.query.weight', 'module.roberta.encoder.layer.4.output.dense.bias', 'module.lm_head.bias', 'module.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'module.roberta.encoder.layer.3.output.LayerNorm.weight', 'module.roberta.encoder.layer.3.attention.self.key.weight', 'module.roberta.encoder.layer.4.attention.self.key.weight', 'module.roberta.encoder.layer.5.attention.self.key.bias', 'module.roberta.encoder.layer.1.output.LayerNorm.weight', 'module.roberta.encoder.layer.5.intermediate.dense.bias', 'module.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'module.roberta.encoder.layer.3.attention.self.query.weight', 'module.roberta.encoder.layer.2.output.LayerNorm.weight', 'module.lm_head.dense.bias', 'module.roberta.encoder.layer.2.attention.self.key.weight', 'module.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'module.roberta.encoder.layer.4.output.dense.weight', 'module.roberta.encoder.layer.4.attention.self.value.bias', 'module.roberta.encoder.layer.3.attention.output.dense.weight', 'module.roberta.embeddings.position_ids', 'module.roberta.encoder.layer.1.attention.self.query.weight', 'module.roberta.encoder.layer.4.attention.output.dense.bias', 'module.roberta.encoder.layer.1.output.dense.bias', 'module.roberta.encoder.layer.3.output.LayerNorm.bias', 'module.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'module.lm_head.dense.weight', 'module.roberta.encoder.layer.2.intermediate.dense.weight', 'module.lm_head.layer_norm.bias', 'module.roberta.encoder.layer.2.attention.self.value.bias', 'module.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'module.roberta.encoder.layer.5.attention.self.value.weight', 'module.roberta.encoder.layer.1.intermediate.dense.weight', 'module.roberta.encoder.layer.2.attention.output.dense.weight', 'module.roberta.embeddings.word_embeddings.weight', 'module.roberta.embeddings.token_type_embeddings.weight', 'module.roberta.encoder.layer.3.attention.self.query.bias', 'module.roberta.encoder.layer.5.attention.self.query.bias', 'module.roberta.encoder.layer.1.output.LayerNorm.bias', 'module.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'module.roberta.encoder.layer.0.attention.self.value.weight', 'module.roberta.encoder.layer.0.output.dense.weight', 'module.roberta.encoder.layer.0.output.LayerNorm.bias', 'module.roberta.encoder.layer.4.attention.self.query.bias', 'module.roberta.encoder.layer.1.output.dense.weight', 'module.roberta.encoder.layer.5.attention.output.dense.weight', 'module.roberta.encoder.layer.2.attention.output.dense.bias', 'module.roberta.encoder.layer.4.intermediate.dense.weight', 'module.roberta.encoder.layer.0.attention.self.key.weight', 'module.roberta.encoder.layer.1.attention.output.dense.weight', 'module.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'module.roberta.encoder.layer.5.output.LayerNorm.weight', 'module.roberta.encoder.layer.3.intermediate.dense.bias', 'module.roberta.embeddings.LayerNorm.weight', 'module.roberta.encoder.layer.0.intermediate.dense.weight', 'module.roberta.encoder.layer.5.output.dense.weight', 'module.roberta.encoder.layer.1.attention.self.key.weight', 'module.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'module.roberta.encoder.layer.1.attention.self.value.bias', 'module.roberta.encoder.layer.2.attention.self.query.weight', 'module.roberta.encoder.layer.4.attention.self.query.weight', 'module.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'module.roberta.encoder.layer.5.intermediate.dense.weight', 'module.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'module.roberta.encoder.layer.3.attention.self.value.weight', 'module.roberta.encoder.layer.3.intermediate.dense.weight', 'module.lm_head.decoder.bias', 'module.roberta.encoder.layer.4.output.LayerNorm.bias', 'module.roberta.encoder.layer.0.attention.self.query.bias', 'module.roberta.encoder.layer.3.output.dense.bias', 'module.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'module.roberta.encoder.layer.2.output.dense.bias', 'module.roberta.encoder.layer.5.attention.self.key.weight', 'module.roberta.encoder.layer.0.output.dense.bias', 'module.roberta.encoder.layer.1.attention.self.query.bias', 'module.roberta.encoder.layer.0.attention.output.dense.weight', 'module.roberta.encoder.layer.5.attention.self.query.weight', 'module.roberta.encoder.layer.2.attention.self.key.bias', 'module.roberta.encoder.layer.4.output.LayerNorm.weight', 'module.roberta.encoder.layer.4.attention.self.value.weight', 'module.roberta.encoder.layer.5.output.LayerNorm.bias', 'module.roberta.encoder.layer.2.output.LayerNorm.bias', 'module.roberta.encoder.layer.2.output.dense.weight', 'module.roberta.encoder.layer.3.output.dense.weight', 'module.roberta.encoder.layer.1.attention.output.dense.bias', 'module.roberta.encoder.layer.2.attention.self.value.weight', 'module.roberta.encoder.layer.1.attention.self.value.weight', 'module.roberta.encoder.layer.3.attention.output.dense.bias', 'module.roberta.encoder.layer.2.attention.self.query.bias', 'module.roberta.encoder.layer.5.output.dense.bias', 'module.roberta.encoder.layer.0.attention.self.value.bias', 'module.lm_head.layer_norm.weight', 'module.roberta.embeddings.position_embeddings.weight', 'module.roberta.encoder.layer.4.intermediate.dense.bias', 'module.roberta.encoder.layer.0.intermediate.dense.bias', 'module.roberta.encoder.layer.3.attention.self.value.bias', 'module.roberta.encoder.layer.5.attention.output.dense.bias', 'module.roberta.encoder.layer.4.attention.output.dense.weight', 'module.roberta.encoder.layer.0.attention.output.dense.bias', 'module.roberta.encoder.layer.3.attention.self.key.bias', 'module.roberta.embeddings.LayerNorm.bias', 'module.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'module.roberta.encoder.layer.0.attention.self.key.bias', 'module.roberta.encoder.layer.4.attention.self.key.bias', 'module.roberta.encoder.layer.5.attention.self.value.bias', 'module.roberta.encoder.layer.2.intermediate.dense.bias', 'module.lm_head.decoder.weight', 'module.roberta.encoder.layer.0.output.LayerNorm.weight', 'module.roberta.encoder.layer.1.attention.self.key.bias', 'module.roberta.encoder.layer.1.intermediate.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at saved_dec7_roberta_a and are newly initialized: ['encoder.layer.5.attention.self.key.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.value.weight', 'lm_head.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.3.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'lm_head.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'lm_head.layer_norm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'lm_head.layer_norm.weight', 'encoder.layer.3.attention.self.key.bias', 'lm_head.dense.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Create a Fill mask pipeline\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"saved_dec7_roberta_a\",\n",
    "    tokenizer=\"code-toknzr-roberta\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'for event in range',\n",
       "  'score': 0.0012233317829668522,\n",
       "  'token': 3565,\n",
       "  'token_str': ' event'},\n",
       " {'sequence': 'for cuml in range',\n",
       "  'score': 0.0010677966056391597,\n",
       "  'token': 6661,\n",
       "  'token_str': ' cuml'},\n",
       " {'sequence': 'for connected in range',\n",
       "  'score': 0.0009085682104341686,\n",
       "  'token': 3004,\n",
       "  'token_str': ' connected'},\n",
       " {'sequence': 'forindexes in range',\n",
       "  'score': 0.0007934276945888996,\n",
       "  'token': 4248,\n",
       "  'token_str': 'indexes'},\n",
       " {'sequence': 'forburger in range',\n",
       "  'score': 0.0007660402334295213,\n",
       "  'token': 4520,\n",
       "  'token_str': 'burger'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"for <mask> in range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'for cuml',\n",
       "  'score': 0.0012699144426733255,\n",
       "  'token': 6661,\n",
       "  'token_str': ' cuml'},\n",
       " {'sequence': 'for event',\n",
       "  'score': 0.0010557093191891909,\n",
       "  'token': 3565,\n",
       "  'token_str': ' event'},\n",
       " {'sequence': 'for connected',\n",
       "  'score': 0.0009457896230742335,\n",
       "  'token': 3004,\n",
       "  'token_str': ' connected'},\n",
       " {'sequence': 'forindexes',\n",
       "  'score': 0.0008830957231111825,\n",
       "  'token': 4248,\n",
       "  'token_str': 'indexes'},\n",
       " {'sequence': 'forburger',\n",
       "  'score': 0.0008391459123231471,\n",
       "  'token': 4520,\n",
       "  'token_str': 'burger'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"for <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'import cuml',\n",
       "  'score': 0.001225362764671445,\n",
       "  'token': 6661,\n",
       "  'token_str': ' cuml'},\n",
       " {'sequence': 'import event',\n",
       "  'score': 0.0010841487674042583,\n",
       "  'token': 3565,\n",
       "  'token_str': ' event'},\n",
       " {'sequence': 'importindexes',\n",
       "  'score': 0.001021808828227222,\n",
       "  'token': 4248,\n",
       "  'token_str': 'indexes'},\n",
       " {'sequence': 'importburger',\n",
       "  'score': 0.0008810916333459318,\n",
       "  'token': 4520,\n",
       "  'token_str': 'burger'},\n",
       " {'sequence': 'import connected',\n",
       "  'score': 0.0007944402750581503,\n",
       "  'token': 3004,\n",
       "  'token_str': ' connected'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"import <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'import sys,gain',\n",
       "  'score': 0.0012259475188329816,\n",
       "  'token': 4980,\n",
       "  'token_str': 'gain'},\n",
       " {'sequence': 'import sys,cover',\n",
       "  'score': 0.0008220626041293144,\n",
       "  'token': 4996,\n",
       "  'token_str': 'cover'},\n",
       " {'sequence': 'import sys, cuml',\n",
       "  'score': 0.0008176953997462988,\n",
       "  'token': 6661,\n",
       "  'token_str': ' cuml'},\n",
       " {'sequence': 'import sys, event',\n",
       "  'score': 0.0007884774822741747,\n",
       "  'token': 3565,\n",
       "  'token_str': ' event'},\n",
       " {'sequence': 'import sys, heap',\n",
       "  'score': 0.000776717730332166,\n",
       "  'token': 566,\n",
       "  'token_str': ' heap'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"import sys, <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'import mathindexes',\n",
       "  'score': 0.0009021040168590844,\n",
       "  'token': 4248,\n",
       "  'token_str': 'indexes'},\n",
       " {'sequence': 'import mathncb',\n",
       "  'score': 0.0008219612063840032,\n",
       "  'token': 5814,\n",
       "  'token_str': 'ncb'},\n",
       " {'sequence': 'import math mean',\n",
       "  'score': 0.0008200139855034649,\n",
       "  'token': 2441,\n",
       "  'token_str': ' mean'},\n",
       " {'sequence': 'import math bcount',\n",
       "  'score': 0.0007356998394243419,\n",
       "  'token': 6329,\n",
       "  'token_str': ' bcount'},\n",
       " {'sequence': 'import mathsetitem',\n",
       "  'score': 0.0007050809217616916,\n",
       "  'token': 5212,\n",
       "  'token_str': 'setitem'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"import math <mask>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'from mathncb',\n",
       "  'score': 0.0008835267508402467,\n",
       "  'token': 5814,\n",
       "  'token_str': 'ncb'},\n",
       " {'sequence': 'from mathindexes',\n",
       "  'score': 0.0008264688076451421,\n",
       "  'token': 4248,\n",
       "  'token_str': 'indexes'},\n",
       " {'sequence': 'from math mean',\n",
       "  'score': 0.0008002515533007681,\n",
       "  'token': 2441,\n",
       "  'token_str': ' mean'},\n",
       " {'sequence': 'from math bcount',\n",
       "  'score': 0.0006951779359951615,\n",
       "  'token': 6329,\n",
       "  'token_str': ' bcount'},\n",
       " {'sequence': 'from mathelations',\n",
       "  'score': 0.0006611536373384297,\n",
       "  'token': 6136,\n",
       "  'token_str': 'elations'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"from math <mask>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorflowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
