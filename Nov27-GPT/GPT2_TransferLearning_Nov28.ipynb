{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Dropout, LSTMCell, RNN, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import pickle\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import requests\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "import argparse\n",
    "from tokenize import tokenize, untokenize, COMMENT, STRING, NEWLINE, ENCODING, ENDMARKER, NL, INDENT, NUMBER\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"Project_CodeNet_Python800.tar.gz\"\n",
    "# data_url = f\"https://dax-cdn.cdn.appdomain.cloud/dax-project-codenet/1.0.0/{file_name}\"\n",
    "\n",
    "# # Download tar archive to local disk\n",
    "# with open(file_name, \"wb\") as f:\n",
    "#     f.write(requests.get(data_url).content)\n",
    "    \n",
    "# # Extract contents of archive to local disk\n",
    "# if os.path.exists(\"data\"):\n",
    "#     shutil.rmtree(\"data\")    \n",
    "# with tarfile.open(file_name) as tfile:\n",
    "#     tfile.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = glob.glob(os.path.join(os.getcwd(),\"Project_CodeNet_Python800/**/*.*\"))\n",
    "# len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lits = json.load(open(\"literals.json\"))\n",
    "\n",
    "def process_string(token, special_chars={\" \": \"U+0020\", \",\": \"U+002C\"}):\n",
    "    str_quote_options = [\"'''\", '\"\"\"', \"'\", '\"']\n",
    "    start_quote = \"\"\n",
    "    end_quote = \"\"\n",
    "    qualifier_regex = r\"^[a-z]+\"\n",
    "    qualifier_match = re.search(qualifier_regex, token)\n",
    "    # string qualifiers like 'r' for regex, 'f' for formatted string, 'b' for bytes, 'u' for unicode, etc (or combination of them)\n",
    "    qualifier = \"\" if not qualifier_match else qualifier_match[0]\n",
    "    # token string without qualifiers\n",
    "    token_string = re.sub(qualifier_regex, \"\", token)\n",
    "    # string literal without quotes\n",
    "    str_lit = token_string\n",
    "    for q in str_quote_options:\n",
    "        if token_string.startswith(q):\n",
    "            start_quote = q\n",
    "            str_lit = str_lit[len(q) :]\n",
    "            if token_string.endswith(q):\n",
    "                end_quote = q\n",
    "                str_lit = str_lit[: -len(q)]\n",
    "            break\n",
    "    # if start_quote in str_quote_options[:2]:\n",
    "    #     return \"\"\n",
    "    for sc in special_chars:\n",
    "        str_lit = str_lit.replace(sc, special_chars[sc])\n",
    "    return (\n",
    "        f\"{qualifier}{start_quote}<STR_LIT:{str_lit}>{end_quote}\"\n",
    "        if str_lit in lits['str']\n",
    "        else f\"{qualifier}{start_quote}<STR_LIT>{end_quote}\"\n",
    "    )\n",
    "\n",
    "def py_tokenize():\n",
    "    file_paths = glob.glob(os.path.join(os.getcwd(),\"Project_CodeNet_Python800/**/*.*\"))\n",
    "    wf = open(os.path.join(os.getcwd(), f\"full_corpus.txt\"), 'w')\n",
    "    local_corpus = []\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            code = open(path).read()\n",
    "            token_gen = tokenize(BytesIO(bytes(code, \"utf8\")).readline)\n",
    "            out_tokens = []\n",
    "            prev_eol = False\n",
    "            for toknum, tokval, _, _, _ in token_gen:\n",
    "                tokval = \" \".join(tokval.split())\n",
    "                if toknum == STRING:\n",
    "                    add_token = process_string(tokval)\n",
    "                    out_tokens.append(add_token)\n",
    "                    prev_eol = False\n",
    "                elif toknum == NUMBER:\n",
    "                    if tokval in lits['num']:\n",
    "                        out_tokens.append(f\"<NUM_LIT:{tokval}>\")\n",
    "                    else:\n",
    "                        out_tokens.append(f\"<NUM_LIT>\")\n",
    "                    prev_eol = False\n",
    "                elif toknum in [NEWLINE, NL]:\n",
    "                    if not prev_eol:\n",
    "                        out_tokens.append(\"<EOL>\")\n",
    "                        prev_eol = True\n",
    "                elif toknum in [COMMENT, INDENT, ENCODING, ENDMARKER] or len(tokval) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    out_tokens.append(tokval)\n",
    "                    prev_eol = False\n",
    "            if out_tokens[0] == \"<EOL>\":\n",
    "                out_tokens = out_tokens[1:]\n",
    "            if out_tokens[-1] == \"<EOL>\":\n",
    "                out_tokens = out_tokens[:-1]\n",
    "        except Exception:\n",
    "            out_tokens = []\n",
    "#         local_corpus.extend((\" \".join(out_tokens)).split('<EOL>'))\n",
    "#         out_tokens = [\"<s>\"] + out_tokens + [\"</s>\"]\n",
    "        out = \" \".join(out_tokens)\n",
    "        local_corpus.append(out)\n",
    "        wf.write(out+\"\\n\")\n",
    "    print(f\"Full Corpus is done\")\n",
    "    wf.close()\n",
    "    return local_corpus\n",
    "\n",
    "def read_corpus():\n",
    "    corpus = py_tokenize()\n",
    "    full_corpus = ''.join(corpus)\n",
    "    corpus_new = []\n",
    "    for code in corpus:\n",
    "        corpus_new.extend(code.split('<EOL>'))\n",
    "        \n",
    "    return pd.DataFrame(corpus_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('full_corpus.txt', encoding='utf8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e = enumerate <EOL> n , * a = map ( int , open ( <NUM_LIT:0> ) . read ( ) . split ( ) ) <EOL> d = [ <NUM_LIT:0> ] <EOL> for j , ( a , i ) in e ( sorted ( ( a , i ) for i , a in e ( a ) ) [ : : - <NUM_LIT:1> ] ) : d = [ d [ <NUM_LIT:0> ] + a * abs ( n - j - i - <NUM_LIT:1> ) ] + [ max ( d [ k ] + a * abs ( n - j + k - i - <NUM_LIT:1> ) , d [ k - <NUM_LIT:1> ] + a * abs ( i - k + <NUM_LIT:1> ) ) for k in range ( <NUM_LIT:1> , j + <NUM_LIT:1> ) ] + [ d [ j ] + a * abs ( i - j ) ] <EOL> print ( max ( d ) )\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_corpus = []\n",
    "# for code in corpus:\n",
    "#     text_corpus.extend(code.split(' <EOL> '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_corpus[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent = corpus[0:int(0.8*len(corpus))]\n",
    "test_sent = corpus[int(0.8*len(corpus)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N = int ( input ( ) ) <EOL> count = <NUM_LIT:0> <EOL> for i in range ( <NUM_LIT:1> , N + <NUM_LIT:1> ) : <EOL> a = str ( i ) <EOL> b = len ( a ) <EOL> if b % <NUM_LIT:2> != <NUM_LIT:0> : <EOL> count += <NUM_LIT:1> <EOL> else : <EOL> count += <NUM_LIT:0> <EOL> print ( count )\\n',\n",
       " 'import math <EOL> N = int ( input ( ) ) <EOL> print ( sum ( [ int ( math . log10 ( x ) ) % <NUM_LIT:2> == <NUM_LIT:0> for x in range ( <NUM_LIT:1> , N + <NUM_LIT:1> ) ] ) )\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"train_sent_byCode.txt\", \"w\") as fp:   #Pickling\n",
    "    fp.write('\\n'.join(train_sent))\n",
    "with open(\"test_sent_byCode.txt\", \"w\") as fp:   #Pickling\n",
    "    fp.write('\\n'.join(test_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print (torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer,OpenAIGPTLMHeadModel,TextDataset,TrainingArguments,Trainer,pipeline,DataCollatorForLanguageModeling, RobertaTokenizer, GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = OpenAIGPTTokenizer.from_pretrained(\"congcongwang/gpt2_medium_fine_tuned_coder\")\n",
    "# tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained('/scratch1/rgoli/gpt_model_nov28/checkpoint-6000/',local_files_only=True)\n",
    "\n",
    "# from custom_tokenise import BPE_token\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "\n",
    "# paths = glob.glob(os.path.join(os.getcwd(),\"Project_CodeNet_Python800/**/*.py\"))\n",
    "# # paths[1:5]\n",
    "# tokenizer = BPE_token()# train the tokenizer model\n",
    "# tokenizer.bpe_train(paths)\n",
    "\n",
    "# # # saving the tokenized data in our specified folder \n",
    "# # save_path = 'tokenized_data'\n",
    "# # tokenizer.save_tokenizer(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /scratch1/rgoli/gpt_model_nov28/checkpoint-6000/ were not used when initializing GPT2LMHeadModel: ['module.transformer.h.1.ln_2.weight', 'module.transformer.h.11.ln_1.bias', 'module.transformer.h.1.attn.c_proj.weight', 'module.transformer.h.4.attn.c_attn.weight', 'module.transformer.h.6.mlp.c_proj.bias', 'module.transformer.ln_f.weight', 'module.transformer.h.8.attn.c_attn.weight', 'module.transformer.h.10.ln_1.bias', 'module.transformer.h.4.attn.bias', 'module.transformer.h.4.attn.c_attn.bias', 'module.transformer.h.10.attn.c_attn.weight', 'module.transformer.h.9.ln_2.weight', 'module.transformer.h.7.attn.c_attn.weight', 'module.transformer.h.11.mlp.c_fc.weight', 'module.transformer.h.2.ln_1.bias', 'module.transformer.h.3.attn.c_attn.weight', 'module.transformer.h.8.ln_1.weight', 'module.transformer.h.6.attn.c_attn.bias', 'module.transformer.h.6.mlp.c_fc.bias', 'module.transformer.h.9.mlp.c_fc.bias', 'module.transformer.h.7.mlp.c_proj.bias', 'module.transformer.h.8.attn.masked_bias', 'module.transformer.h.10.ln_2.bias', 'module.transformer.h.2.attn.c_attn.weight', 'module.transformer.h.0.attn.masked_bias', 'module.transformer.h.8.attn.c_proj.weight', 'module.transformer.h.1.attn.c_attn.bias', 'module.transformer.h.2.mlp.c_fc.bias', 'module.transformer.h.11.attn.masked_bias', 'module.transformer.h.5.mlp.c_fc.bias', 'module.transformer.h.3.ln_2.bias', 'module.transformer.h.7.mlp.c_fc.bias', 'module.transformer.h.9.attn.c_attn.weight', 'module.transformer.h.10.attn.c_attn.bias', 'module.transformer.h.10.attn.c_proj.bias', 'module.transformer.h.9.ln_2.bias', 'module.transformer.h.8.ln_2.bias', 'module.transformer.h.10.ln_2.weight', 'module.transformer.wpe.weight', 'module.transformer.h.9.attn.c_proj.weight', 'module.transformer.h.11.mlp.c_fc.bias', 'module.transformer.h.8.ln_1.bias', 'module.transformer.h.5.attn.c_attn.weight', 'module.transformer.h.3.attn.bias', 'module.transformer.h.5.attn.c_proj.bias', 'module.transformer.h.3.ln_1.bias', 'module.transformer.h.1.ln_1.bias', 'module.transformer.h.0.attn.c_attn.bias', 'module.transformer.h.4.attn.masked_bias', 'module.transformer.h.9.attn.c_attn.bias', 'module.transformer.h.11.attn.bias', 'module.transformer.h.11.mlp.c_proj.bias', 'module.transformer.h.11.attn.c_attn.bias', 'module.transformer.h.0.attn.c_proj.bias', 'module.transformer.h.1.attn.c_attn.weight', 'module.transformer.h.4.mlp.c_proj.bias', 'module.transformer.h.3.mlp.c_proj.weight', 'module.transformer.h.5.mlp.c_proj.weight', 'module.transformer.h.7.ln_1.bias', 'module.transformer.h.10.attn.masked_bias', 'module.transformer.h.10.attn.c_proj.weight', 'module.transformer.h.5.attn.bias', 'module.transformer.h.8.attn.c_attn.bias', 'module.transformer.h.7.mlp.c_proj.weight', 'module.transformer.h.0.ln_1.bias', 'module.transformer.h.9.mlp.c_proj.weight', 'module.transformer.h.3.mlp.c_fc.weight', 'module.transformer.h.4.attn.c_proj.bias', 'module.transformer.h.2.attn.c_attn.bias', 'module.transformer.h.5.attn.masked_bias', 'module.transformer.h.5.ln_2.weight', 'module.transformer.h.3.ln_2.weight', 'module.transformer.h.8.mlp.c_proj.weight', 'module.transformer.h.1.ln_1.weight', 'module.transformer.h.0.attn.bias', 'module.transformer.h.10.attn.bias', 'module.transformer.h.11.attn.c_proj.weight', 'module.transformer.h.2.attn.masked_bias', 'module.transformer.h.7.attn.c_proj.weight', 'module.transformer.h.7.ln_2.weight', 'module.transformer.h.8.ln_2.weight', 'module.transformer.h.5.mlp.c_fc.weight', 'module.transformer.h.5.attn.c_attn.bias', 'module.transformer.h.7.attn.bias', 'module.transformer.h.9.mlp.c_fc.weight', 'module.transformer.h.8.mlp.c_fc.weight', 'module.transformer.h.9.attn.masked_bias', 'module.transformer.h.7.ln_2.bias', 'module.transformer.h.0.mlp.c_fc.weight', 'module.transformer.h.7.attn.c_proj.bias', 'module.transformer.h.0.mlp.c_proj.bias', 'module.transformer.h.1.mlp.c_fc.weight', 'module.transformer.h.1.attn.masked_bias', 'module.transformer.h.0.mlp.c_proj.weight', 'module.transformer.h.2.ln_2.bias', 'module.transformer.h.7.ln_1.weight', 'module.transformer.h.5.ln_1.bias', 'module.transformer.h.6.ln_1.bias', 'module.transformer.h.6.mlp.c_fc.weight', 'module.transformer.h.0.attn.c_attn.weight', 'module.transformer.h.6.attn.c_proj.bias', 'module.transformer.h.6.ln_2.bias', 'module.transformer.h.4.ln_2.weight', 'module.transformer.h.2.ln_2.weight', 'module.transformer.h.6.mlp.c_proj.weight', 'module.transformer.h.10.mlp.c_fc.bias', 'module.transformer.h.3.attn.c_attn.bias', 'module.transformer.h.2.mlp.c_proj.weight', 'module.transformer.h.5.mlp.c_proj.bias', 'module.transformer.h.9.attn.bias', 'module.transformer.h.11.attn.c_proj.bias', 'module.lm_head.weight', 'module.transformer.h.4.ln_1.weight', 'module.transformer.h.6.attn.c_proj.weight', 'module.transformer.h.3.attn.c_proj.weight', 'module.transformer.h.9.ln_1.weight', 'module.transformer.h.2.mlp.c_fc.weight', 'module.transformer.wte.weight', 'module.transformer.h.3.ln_1.weight', 'module.transformer.h.8.attn.bias', 'module.transformer.h.6.ln_1.weight', 'module.transformer.h.10.ln_1.weight', 'module.transformer.h.0.mlp.c_fc.bias', 'module.transformer.h.7.attn.masked_bias', 'module.transformer.h.0.ln_1.weight', 'module.transformer.ln_f.bias', 'module.transformer.h.2.attn.c_proj.bias', 'module.transformer.h.2.attn.bias', 'module.transformer.h.7.attn.c_attn.bias', 'module.transformer.h.9.attn.c_proj.bias', 'module.transformer.h.2.ln_1.weight', 'module.transformer.h.6.ln_2.weight', 'module.transformer.h.0.attn.c_proj.weight', 'module.transformer.h.8.attn.c_proj.bias', 'module.transformer.h.3.mlp.c_proj.bias', 'module.transformer.h.10.mlp.c_proj.bias', 'module.transformer.h.4.attn.c_proj.weight', 'module.transformer.h.9.mlp.c_proj.bias', 'module.transformer.h.5.ln_1.weight', 'module.transformer.h.1.attn.bias', 'module.transformer.h.5.ln_2.bias', 'module.transformer.h.7.mlp.c_fc.weight', 'module.transformer.h.3.attn.c_proj.bias', 'module.transformer.h.4.mlp.c_proj.weight', 'module.transformer.h.1.mlp.c_proj.bias', 'module.transformer.h.3.attn.masked_bias', 'module.transformer.h.6.attn.c_attn.weight', 'module.transformer.h.1.mlp.c_proj.weight', 'module.transformer.h.11.ln_2.bias', 'module.transformer.h.4.mlp.c_fc.weight', 'module.transformer.h.3.mlp.c_fc.bias', 'module.transformer.h.10.mlp.c_fc.weight', 'module.transformer.h.0.ln_2.weight', 'module.transformer.h.4.mlp.c_fc.bias', 'module.transformer.h.11.mlp.c_proj.weight', 'module.transformer.h.8.mlp.c_fc.bias', 'module.transformer.h.5.attn.c_proj.weight', 'module.transformer.h.11.ln_1.weight', 'module.transformer.h.11.attn.c_attn.weight', 'module.transformer.h.4.ln_1.bias', 'module.transformer.h.2.mlp.c_proj.bias', 'module.transformer.h.6.attn.bias', 'module.transformer.h.0.ln_2.bias', 'module.transformer.h.1.ln_2.bias', 'module.transformer.h.8.mlp.c_proj.bias', 'module.transformer.h.6.attn.masked_bias', 'module.transformer.h.1.attn.c_proj.bias', 'module.transformer.h.9.ln_1.bias', 'module.transformer.h.4.ln_2.bias', 'module.transformer.h.2.attn.c_proj.weight', 'module.transformer.h.1.mlp.c_fc.bias', 'module.transformer.h.11.ln_2.weight', 'module.transformer.h.10.mlp.c_proj.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /scratch1/rgoli/gpt_model_nov28/checkpoint-6000/ and are newly initialized: ['h.6.attn.c_proj.bias', 'wte.weight', 'h.10.ln_2.weight', 'h.0.mlp.c_proj.weight', 'h.3.attn.c_proj.weight', 'h.5.attn.c_attn.weight', 'h.3.mlp.c_proj.weight', 'h.1.ln_2.bias', 'h.11.ln_2.bias', 'h.3.mlp.c_fc.weight', 'h.7.attn.c_proj.weight', 'h.0.attn.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.6.attn.c_attn.weight', 'h.2.ln_2.weight', 'h.10.attn.c_proj.bias', 'h.10.attn.c_attn.weight', 'h.4.mlp.c_proj.bias', 'h.8.mlp.c_fc.bias', 'h.5.mlp.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.11.attn.c_proj.bias', 'h.0.mlp.c_proj.bias', 'h.1.mlp.c_proj.bias', 'h.11.mlp.c_proj.weight', 'h.11.attn.c_proj.weight', 'h.11.attn.c_attn.weight', 'h.10.ln_1.weight', 'h.8.ln_1.weight', 'h.11.mlp.c_fc.bias', 'h.7.ln_2.weight', 'h.0.mlp.c_fc.weight', 'h.6.ln_2.weight', 'h.9.ln_1.weight', 'h.7.attn.c_proj.bias', 'h.9.ln_2.weight', 'ln_f.weight', 'h.2.mlp.c_fc.weight', 'h.3.mlp.c_fc.bias', 'h.8.ln_1.bias', 'h.10.mlp.c_fc.weight', 'h.11.ln_2.weight', 'h.10.mlp.c_fc.bias', 'h.1.attn.c_proj.bias', 'h.10.ln_1.bias', 'h.0.ln_2.weight', 'h.3.ln_2.bias', 'h.5.ln_1.bias', 'h.11.ln_1.bias', 'h.9.mlp.c_fc.weight', 'h.7.mlp.c_fc.bias', 'h.9.mlp.c_proj.bias', 'h.4.attn.c_attn.weight', 'h.5.mlp.c_fc.weight', 'h.2.ln_1.weight', 'h.4.ln_2.bias', 'h.8.mlp.c_fc.weight', 'h.10.attn.c_proj.weight', 'ln_f.bias', 'h.5.attn.c_proj.bias', 'h.0.attn.c_attn.weight', 'h.3.ln_1.weight', 'h.8.attn.c_proj.weight', 'h.3.ln_1.bias', 'h.1.ln_1.bias', 'h.3.ln_2.weight', 'h.3.mlp.c_proj.bias', 'h.5.attn.c_proj.weight', 'h.6.ln_2.bias', 'h.10.ln_2.bias', 'h.3.attn.c_proj.bias', 'h.2.attn.c_proj.weight', 'h.2.mlp.c_proj.bias', 'h.1.mlp.c_proj.weight', 'h.7.mlp.c_fc.weight', 'h.7.mlp.c_proj.weight', 'h.6.ln_1.bias', 'h.5.mlp.c_fc.bias', 'h.8.ln_2.weight', 'h.9.attn.c_proj.weight', 'h.9.mlp.c_fc.bias', 'h.5.ln_1.weight', 'h.2.mlp.c_fc.bias', 'h.9.ln_1.bias', 'h.7.ln_2.bias', 'h.10.mlp.c_proj.bias', 'h.2.attn.c_attn.weight', 'h.6.mlp.c_proj.weight', 'h.8.mlp.c_proj.weight', 'h.1.attn.c_attn.weight', 'h.1.ln_2.weight', 'h.1.mlp.c_fc.bias', 'h.5.mlp.c_proj.bias', 'h.4.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.6.ln_1.weight', 'h.1.mlp.c_fc.weight', 'h.2.ln_1.bias', 'h.11.mlp.c_fc.weight', 'h.2.ln_2.bias', 'h.8.mlp.c_proj.bias', 'h.7.ln_1.weight', 'h.8.ln_2.bias', 'h.0.ln_1.weight', 'h.5.ln_2.weight', 'h.7.ln_1.bias', 'h.6.attn.c_proj.weight', 'h.1.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.7.mlp.c_proj.bias', 'h.4.ln_1.bias', 'h.8.attn.c_proj.bias', 'h.5.ln_2.bias', 'h.10.mlp.c_proj.weight', 'h.4.mlp.c_fc.bias', 'h.6.mlp.c_fc.bias', 'h.2.mlp.c_proj.weight', 'h.4.attn.c_proj.weight', 'h.6.mlp.c_proj.bias', 'h.11.ln_1.weight', 'h.6.mlp.c_fc.weight', 'h.7.attn.c_attn.weight', 'wpe.weight', 'h.0.ln_2.bias', 'h.4.ln_1.weight', 'h.9.ln_2.bias', 'h.4.mlp.c_proj.weight', 'h.2.attn.c_proj.bias', 'h.0.attn.c_proj.bias', 'h.3.attn.c_attn.weight', 'h.8.attn.c_attn.weight', 'h.11.mlp.c_proj.bias', 'h.4.mlp.c_fc.weight', 'h.9.attn.c_attn.weight', 'h.1.ln_1.weight', 'h.9.mlp.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('/scratch1/rgoli/gpt_model_nov28/checkpoint-6000/',local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0,1,2,3], dim=0)\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 50257, max sequence length: 1024\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary size: %d, max sequence length: %d' % (tokenizer.vocab_size, tokenizer.model_max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   68,   796, 27056,   378,  1279,    36,  3535,    29,   299,   837,\n",
      "          1635,   257,   796,  3975,   357,   493,   837,  1280,   357,  1279,\n",
      "         41359,    62,    43,  2043,    25,    15,    29,  1267,   764,  1100,\n",
      "           357,  1267,   764,  6626,   357,  1267,  1267,  1279,    36,  3535,\n",
      "            29,   288,   796,   685,  1279, 41359,    62,    43,  2043,    25,\n",
      "            15,    29,  2361,  1279,    36,  3535,    29,   329,   474,   837,\n",
      "           357,   257,   837,  1312,  1267,   287,   304,   357, 23243,   357,\n",
      "           357,   257,   837,  1312,  1267,   329,  1312,   837,   257,   287,\n",
      "           304,   357,   257,  1267,  1267,   685,  1058,  1058,   532,  1279,\n",
      "         41359,    62,    43,  2043,    25,    16,    29,  2361,  1267,  1058,\n",
      "           288,   796,   685,   288,   685,  1279, 41359,    62,    43,  2043,\n",
      "            25,    15,    29,  2361,  1343,   257,  1635,  2352,   357,   299,\n",
      "           532,   474,   532,  1312,   532,  1279, 41359,    62,    43,  2043,\n",
      "            25,    16,    29,  1267,  2361,  1343,   685,  3509,   357,   288,\n",
      "           685,   479,  2361,  1343,   257,  1635,  2352,   357,   299,   532,\n",
      "           474,  1343,   479,   532,  1312,   532,  1279, 41359,    62,    43,\n",
      "          2043,    25,    16,    29,  1267,   837,   288,   685,   479,   532,\n",
      "          1279, 41359,    62,    43,  2043,    25,    16,    29,  2361,  1343,\n",
      "           257,  1635,  2352,   357,  1312,   532,   479,  1343,  1279, 41359,\n",
      "            62,    43,  2043,    25,    16,    29,  1267,  1267,   329,   479,\n",
      "           287,  2837,   357,  1279, 41359,    62,    43,  2043,    25,    16,\n",
      "            29,   837,   474,  1343,  1279, 41359,    62,    43,  2043,    25,\n",
      "            16,    29,  1267,  2361,  1343,   685,   288,   685,   474,  2361,\n",
      "          1343,   257,  1635,  2352,   357,  1312,   532,   474,  1267,  2361,\n",
      "          1279,    36,  3535,    29,  3601,   357,  3509,   357,   288,  1267,\n",
      "          1267,   198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(train_sent[0], return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'Ġ=', 'Ġenumer', 'ate', 'Ġ<', 'E', 'OL', '>', 'Ġn', 'Ġ,', 'Ġ*', 'Ġa', 'Ġ=', 'Ġmap', 'Ġ(', 'Ġint', 'Ġ,', 'Ġopen', 'Ġ(', 'Ġ<', 'NUM', '_', 'L', 'IT', ':', '0', '>', 'Ġ)', 'Ġ.', 'Ġread', 'Ġ(', 'Ġ)', 'Ġ.', 'Ġsplit', 'Ġ(', 'Ġ)', 'Ġ)', 'Ġ<', 'E', 'OL', '>', 'Ġd', 'Ġ=', 'Ġ[', 'Ġ<', 'NUM', '_', 'L', 'IT', ':', '0', '>', 'Ġ]', 'Ġ<', 'E', 'OL', '>', 'Ġfor', 'Ġj', 'Ġ,', 'Ġ(', 'Ġa', 'Ġ,', 'Ġi', 'Ġ)', 'Ġin', 'Ġe', 'Ġ(', 'Ġsorted', 'Ġ(', 'Ġ(', 'Ġa', 'Ġ,', 'Ġi', 'Ġ)', 'Ġfor', 'Ġi', 'Ġ,', 'Ġa', 'Ġin', 'Ġe', 'Ġ(', 'Ġa', 'Ġ)', 'Ġ)', 'Ġ[', 'Ġ:', 'Ġ:', 'Ġ-', 'Ġ<', 'NUM', '_', 'L', 'IT', ':', '1', '>', 'Ġ]', 'Ġ)', 'Ġ:', 'Ġd', 'Ġ=', 'Ġ[', 'Ġd', 'Ġ[', 'Ġ<', 'NUM', '_', 'L', 'IT', ':', '0', '>', 'Ġ]', 'Ġ+', 'Ġa', 'Ġ*', 'Ġabs', 'Ġ(', 'Ġn', 'Ġ-', 'Ġj', 'Ġ-', 'Ġi', 'Ġ-', 'Ġ<', 'NUM', '_', 'L', 'IT', ':', '1', '>', 'Ġ)', 'Ġ]', 'Ġ+', 'Ġ[', 'Ġmax', 'Ġ(', 'Ġd', 'Ġ[', 'Ġk', 'Ġ]', 'Ġ+', 'Ġa', 'Ġ*', 'Ġabs', 'Ġ(', 'Ġn', 'Ġ-', 'Ġj', 'Ġ+', 'Ġk', 'Ġ-', 'Ġi', 'Ġ-', 'Ġ<', 'NUM', '_', 'L', 'IT', ':', '1', '>', 'Ġ)', 'Ġ,', 'Ġd', 'Ġ[', 'Ġk', 'Ġ-', 'Ġ<', 'NUM', '_', 'L', 'IT', ':', '1', '>', 'Ġ]', 'Ġ+', 'Ġa', 'Ġ*', 'Ġabs', 'Ġ(', 'Ġi', 'Ġ-', 'Ġk', 'Ġ+', 'Ġ<', 'NUM', '_', 'L', 'IT', ':', '1', '>', 'Ġ)', 'Ġ)', 'Ġfor', 'Ġk', 'Ġin', 'Ġrange', 'Ġ(', 'Ġ<', 'NUM', '_', 'L', 'IT', ':', '1', '>', 'Ġ,', 'Ġj', 'Ġ+', 'Ġ<', 'NUM', '_', 'L', 'IT', ':', '1', '>', 'Ġ)', 'Ġ]', 'Ġ+', 'Ġ[', 'Ġd', 'Ġ[', 'Ġj', 'Ġ]', 'Ġ+', 'Ġa', 'Ġ*', 'Ġabs', 'Ġ(', 'Ġi', 'Ġ-', 'Ġj', 'Ġ)', 'Ġ]', 'Ġ<', 'E', 'OL', '>', 'Ġprint', 'Ġ(', 'Ġmax', 'Ġ(', 'Ġd', 'Ġ)', 'Ġ)', 'Ċ']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([   68,   796, 27056,   378,  1279,    36,  3535,    29,   299,   837,\n",
    "          1635,   257,   796,  3975,   357,   493,   837,  1280,   357,  1279,\n",
    "         41359,    62,    43,  2043,    25,    15,    29,  1267,   764,  1100,\n",
    "           357,  1267,   764,  6626,   357,  1267,  1267,  1279,    36,  3535,\n",
    "            29,   288,   796,   685,  1279, 41359,    62,    43,  2043,    25,\n",
    "            15,    29,  2361,  1279,    36,  3535,    29,   329,   474,   837,\n",
    "           357,   257,   837,  1312,  1267,   287,   304,   357, 23243,   357,\n",
    "           357,   257,   837,  1312,  1267,   329,  1312,   837,   257,   287,\n",
    "           304,   357,   257,  1267,  1267,   685,  1058,  1058,   532,  1279,\n",
    "         41359,    62,    43,  2043,    25,    16,    29,  2361,  1267,  1058,\n",
    "           288,   796,   685,   288,   685,  1279, 41359,    62,    43,  2043,\n",
    "            25,    15,    29,  2361,  1343,   257,  1635,  2352,   357,   299,\n",
    "           532,   474,   532,  1312,   532,  1279, 41359,    62,    43,  2043,\n",
    "            25,    16,    29,  1267,  2361,  1343,   685,  3509,   357,   288,\n",
    "           685,   479,  2361,  1343,   257,  1635,  2352,   357,   299,   532,\n",
    "           474,  1343,   479,   532,  1312,   532,  1279, 41359,    62,    43,\n",
    "          2043,    25,    16,    29,  1267,   837,   288,   685,   479,   532,\n",
    "          1279, 41359,    62,    43,  2043,    25,    16,    29,  2361,  1343,\n",
    "           257,  1635,  2352,   357,  1312,   532,   479,  1343,  1279, 41359,\n",
    "            62,    43,  2043,    25,    16,    29,  1267,  1267,   329,   479,\n",
    "           287,  2837,   357,  1279, 41359,    62,    43,  2043,    25,    16,\n",
    "            29,   837,   474,  1343,  1279, 41359,    62,    43,  2043,    25,\n",
    "            16,    29,  1267,  2361,  1343,   685,   288,   685,   474,  2361,\n",
    "          1343,   257,  1635,  2352,   357,  1312,   532,   474,  1267,  2361,\n",
    "          1279,    36,  3535,    29,  3601,   357,  3509,   357,   288,  1267,\n",
    "          1267,   198]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='train_sent.txt',\n",
    "    overwrite_cache=True,\n",
    "    block_size=128)\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='test_sent.txt',\n",
    "    overwrite_cache=True,\n",
    "    block_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = '/scratch1/rgoli/gpt_model_nov28', \n",
    "    overwrite_output_dir = True, \n",
    "    per_device_train_batch_size = 24, \n",
    "    per_device_eval_batch_size = 24, \n",
    "    learning_rate = 5e-4, \n",
    "    num_train_epochs = 3,\n",
    "    save_steps=3000,\n",
    "    logging_steps=3000,\n",
    "    save_total_limit=2\n",
    ")\n",
    "# Initializing the trainer class object that will do the training\n",
    "# here the data collator will generate the batch of size 64 of train and test data\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 322156\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 24\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 96\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10068\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrohangoli\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rohangoli/huggingface/runs/3avdq000\" target=\"_blank\">/scratch1/rgoli/gpt_model_nov28</a></strong> to <a href=\"https://wandb.ai/rohangoli/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10068' max='10068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10068/10068 59:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.991200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.551000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /scratch1/rgoli/gpt_model_nov28/checkpoint-3000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model_nov28/checkpoint-3000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model_nov28/checkpoint-3000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model_nov28/checkpoint-6000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model_nov28/checkpoint-6000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model_nov28/checkpoint-6000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model_nov28/checkpoint-9000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model_nov28/checkpoint-9000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model_nov28/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [/scratch1/rgoli/gpt_model_nov28/checkpoint-3000] due to args.save_total_limit\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10068, training_loss=0.7045274263911651, metrics={'train_runtime': 3577.9642, 'train_samples_per_second': 270.117, 'train_steps_per_second': 2.814, 'total_flos': 0.0, 'train_loss': 0.7045274263911651, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model for 3 epochs\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_nov28\n",
      "Configuration saved in ./saved_nov28/config.json\n",
      "Model weights saved in ./saved_nov28/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./saved_nov28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 80189\n",
      "  Batch size = 96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='836' max='836' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [836/836 02:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.758241593837738,\n",
       " 'eval_runtime': 125.4956,\n",
       " 'eval_samples_per_second': 638.979,\n",
       " 'eval_steps_per_second': 6.662,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating on Test data\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved_nov28/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"/scratch1/rgoli/gpt_model_nov28/checkpoint-6000/\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file saved_nov28/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"/scratch1/rgoli/gpt_model_nov28/checkpoint-6000/\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file saved_nov28/pytorch_model.bin\n",
      "Some weights of the model checkpoint at saved_nov28 were not used when initializing GPT2LMHeadModel: ['module.transformer.h.1.ln_2.weight', 'module.transformer.h.11.ln_1.bias', 'module.transformer.h.1.attn.c_proj.weight', 'module.transformer.h.4.attn.c_attn.weight', 'module.transformer.h.6.mlp.c_proj.bias', 'module.transformer.ln_f.weight', 'module.transformer.h.8.attn.c_attn.weight', 'module.transformer.h.10.ln_1.bias', 'module.transformer.h.4.attn.bias', 'module.transformer.h.4.attn.c_attn.bias', 'module.transformer.h.10.attn.c_attn.weight', 'module.transformer.h.9.ln_2.weight', 'module.transformer.h.7.attn.c_attn.weight', 'module.transformer.h.11.mlp.c_fc.weight', 'module.transformer.h.2.ln_1.bias', 'module.transformer.h.3.attn.c_attn.weight', 'module.transformer.h.8.ln_1.weight', 'module.transformer.h.6.attn.c_attn.bias', 'module.transformer.h.6.mlp.c_fc.bias', 'module.transformer.h.9.mlp.c_fc.bias', 'module.transformer.h.7.mlp.c_proj.bias', 'module.transformer.h.8.attn.masked_bias', 'module.transformer.h.10.ln_2.bias', 'module.transformer.h.2.attn.c_attn.weight', 'module.transformer.h.0.attn.masked_bias', 'module.transformer.h.8.attn.c_proj.weight', 'module.transformer.h.1.attn.c_attn.bias', 'module.transformer.h.2.mlp.c_fc.bias', 'module.transformer.h.11.attn.masked_bias', 'module.transformer.h.5.mlp.c_fc.bias', 'module.transformer.h.3.ln_2.bias', 'module.transformer.h.7.mlp.c_fc.bias', 'module.transformer.h.9.attn.c_attn.weight', 'module.transformer.h.10.attn.c_attn.bias', 'module.transformer.h.10.attn.c_proj.bias', 'module.transformer.h.9.ln_2.bias', 'module.transformer.h.8.ln_2.bias', 'module.transformer.h.10.ln_2.weight', 'module.transformer.wpe.weight', 'module.transformer.h.9.attn.c_proj.weight', 'module.transformer.h.11.mlp.c_fc.bias', 'module.transformer.h.8.ln_1.bias', 'module.transformer.h.5.attn.c_attn.weight', 'module.transformer.h.3.attn.bias', 'module.transformer.h.5.attn.c_proj.bias', 'module.transformer.h.3.ln_1.bias', 'module.transformer.h.1.ln_1.bias', 'module.transformer.h.0.attn.c_attn.bias', 'module.transformer.h.4.attn.masked_bias', 'module.transformer.h.9.attn.c_attn.bias', 'module.transformer.h.11.attn.bias', 'module.transformer.h.11.mlp.c_proj.bias', 'module.transformer.h.11.attn.c_attn.bias', 'module.transformer.h.0.attn.c_proj.bias', 'module.transformer.h.1.attn.c_attn.weight', 'module.transformer.h.4.mlp.c_proj.bias', 'module.transformer.h.3.mlp.c_proj.weight', 'module.transformer.h.5.mlp.c_proj.weight', 'module.transformer.h.7.ln_1.bias', 'module.transformer.h.10.attn.masked_bias', 'module.transformer.h.10.attn.c_proj.weight', 'module.transformer.h.5.attn.bias', 'module.transformer.h.8.attn.c_attn.bias', 'module.transformer.h.7.mlp.c_proj.weight', 'module.transformer.h.0.ln_1.bias', 'module.transformer.h.9.mlp.c_proj.weight', 'module.transformer.h.3.mlp.c_fc.weight', 'module.transformer.h.4.attn.c_proj.bias', 'module.transformer.h.2.attn.c_attn.bias', 'module.transformer.h.5.attn.masked_bias', 'module.transformer.h.5.ln_2.weight', 'module.transformer.h.3.ln_2.weight', 'module.transformer.h.8.mlp.c_proj.weight', 'module.transformer.h.1.ln_1.weight', 'module.transformer.h.0.attn.bias', 'module.transformer.h.10.attn.bias', 'module.transformer.h.11.attn.c_proj.weight', 'module.transformer.h.2.attn.masked_bias', 'module.transformer.h.7.attn.c_proj.weight', 'module.transformer.h.7.ln_2.weight', 'module.transformer.h.8.ln_2.weight', 'module.transformer.h.5.mlp.c_fc.weight', 'module.transformer.h.5.attn.c_attn.bias', 'module.transformer.h.7.attn.bias', 'module.transformer.h.9.mlp.c_fc.weight', 'module.transformer.h.8.mlp.c_fc.weight', 'module.transformer.h.9.attn.masked_bias', 'module.transformer.h.7.ln_2.bias', 'module.transformer.h.0.mlp.c_fc.weight', 'module.transformer.h.7.attn.c_proj.bias', 'module.transformer.h.0.mlp.c_proj.bias', 'module.transformer.h.1.mlp.c_fc.weight', 'module.transformer.h.1.attn.masked_bias', 'module.transformer.h.0.mlp.c_proj.weight', 'module.transformer.h.2.ln_2.bias', 'module.transformer.h.7.ln_1.weight', 'module.transformer.h.5.ln_1.bias', 'module.transformer.h.6.ln_1.bias', 'module.transformer.h.6.mlp.c_fc.weight', 'module.transformer.h.0.attn.c_attn.weight', 'module.transformer.h.6.attn.c_proj.bias', 'module.transformer.h.6.ln_2.bias', 'module.transformer.h.4.ln_2.weight', 'module.transformer.h.2.ln_2.weight', 'module.transformer.h.6.mlp.c_proj.weight', 'module.transformer.h.10.mlp.c_fc.bias', 'module.transformer.h.3.attn.c_attn.bias', 'module.transformer.h.2.mlp.c_proj.weight', 'module.transformer.h.5.mlp.c_proj.bias', 'module.transformer.h.9.attn.bias', 'module.transformer.h.11.attn.c_proj.bias', 'module.lm_head.weight', 'module.transformer.h.4.ln_1.weight', 'module.transformer.h.6.attn.c_proj.weight', 'module.transformer.h.3.attn.c_proj.weight', 'module.transformer.h.9.ln_1.weight', 'module.transformer.h.2.mlp.c_fc.weight', 'module.transformer.wte.weight', 'module.transformer.h.3.ln_1.weight', 'module.transformer.h.8.attn.bias', 'module.transformer.h.6.ln_1.weight', 'module.transformer.h.10.ln_1.weight', 'module.transformer.h.0.mlp.c_fc.bias', 'module.transformer.h.7.attn.masked_bias', 'module.transformer.h.0.ln_1.weight', 'module.transformer.ln_f.bias', 'module.transformer.h.2.attn.c_proj.bias', 'module.transformer.h.2.attn.bias', 'module.transformer.h.7.attn.c_attn.bias', 'module.transformer.h.9.attn.c_proj.bias', 'module.transformer.h.2.ln_1.weight', 'module.transformer.h.6.ln_2.weight', 'module.transformer.h.0.attn.c_proj.weight', 'module.transformer.h.8.attn.c_proj.bias', 'module.transformer.h.3.mlp.c_proj.bias', 'module.transformer.h.10.mlp.c_proj.bias', 'module.transformer.h.4.attn.c_proj.weight', 'module.transformer.h.9.mlp.c_proj.bias', 'module.transformer.h.5.ln_1.weight', 'module.transformer.h.1.attn.bias', 'module.transformer.h.5.ln_2.bias', 'module.transformer.h.7.mlp.c_fc.weight', 'module.transformer.h.3.attn.c_proj.bias', 'module.transformer.h.4.mlp.c_proj.weight', 'module.transformer.h.1.mlp.c_proj.bias', 'module.transformer.h.3.attn.masked_bias', 'module.transformer.h.6.attn.c_attn.weight', 'module.transformer.h.1.mlp.c_proj.weight', 'module.transformer.h.11.ln_2.bias', 'module.transformer.h.4.mlp.c_fc.weight', 'module.transformer.h.3.mlp.c_fc.bias', 'module.transformer.h.10.mlp.c_fc.weight', 'module.transformer.h.0.ln_2.weight', 'module.transformer.h.4.mlp.c_fc.bias', 'module.transformer.h.11.mlp.c_proj.weight', 'module.transformer.h.8.mlp.c_fc.bias', 'module.transformer.h.5.attn.c_proj.weight', 'module.transformer.h.11.ln_1.weight', 'module.transformer.h.11.attn.c_attn.weight', 'module.transformer.h.4.ln_1.bias', 'module.transformer.h.2.mlp.c_proj.bias', 'module.transformer.h.6.attn.bias', 'module.transformer.h.0.ln_2.bias', 'module.transformer.h.1.ln_2.bias', 'module.transformer.h.8.mlp.c_proj.bias', 'module.transformer.h.6.attn.masked_bias', 'module.transformer.h.1.attn.c_proj.bias', 'module.transformer.h.9.ln_1.bias', 'module.transformer.h.4.ln_2.bias', 'module.transformer.h.2.attn.c_proj.weight', 'module.transformer.h.1.mlp.c_fc.bias', 'module.transformer.h.11.ln_2.weight', 'module.transformer.h.10.mlp.c_proj.weight']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at saved_nov28 and are newly initialized: ['h.6.attn.c_proj.bias', 'wte.weight', 'h.10.ln_2.weight', 'h.0.mlp.c_proj.weight', 'h.3.attn.c_proj.weight', 'h.5.attn.c_attn.weight', 'h.3.mlp.c_proj.weight', 'h.1.ln_2.bias', 'h.11.ln_2.bias', 'h.3.mlp.c_fc.weight', 'h.7.attn.c_proj.weight', 'h.0.attn.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.6.attn.c_attn.weight', 'h.2.ln_2.weight', 'h.10.attn.c_proj.bias', 'h.10.attn.c_attn.weight', 'h.4.mlp.c_proj.bias', 'h.8.mlp.c_fc.bias', 'h.5.mlp.c_proj.weight', 'h.9.attn.c_proj.bias', 'h.11.attn.c_proj.bias', 'h.0.mlp.c_proj.bias', 'h.1.mlp.c_proj.bias', 'h.11.mlp.c_proj.weight', 'h.11.attn.c_proj.weight', 'h.11.attn.c_attn.weight', 'h.10.ln_1.weight', 'h.8.ln_1.weight', 'h.11.mlp.c_fc.bias', 'h.7.ln_2.weight', 'h.0.mlp.c_fc.weight', 'h.6.ln_2.weight', 'h.9.ln_1.weight', 'h.7.attn.c_proj.bias', 'h.9.ln_2.weight', 'ln_f.weight', 'h.2.mlp.c_fc.weight', 'h.3.mlp.c_fc.bias', 'h.8.ln_1.bias', 'h.10.mlp.c_fc.weight', 'h.11.ln_2.weight', 'h.10.mlp.c_fc.bias', 'h.1.attn.c_proj.bias', 'h.10.ln_1.bias', 'h.0.ln_2.weight', 'h.3.ln_2.bias', 'h.5.ln_1.bias', 'h.11.ln_1.bias', 'h.9.mlp.c_fc.weight', 'h.7.mlp.c_fc.bias', 'h.9.mlp.c_proj.bias', 'h.4.attn.c_attn.weight', 'h.5.mlp.c_fc.weight', 'h.2.ln_1.weight', 'h.4.ln_2.bias', 'h.8.mlp.c_fc.weight', 'h.10.attn.c_proj.weight', 'ln_f.bias', 'h.5.attn.c_proj.bias', 'h.0.attn.c_attn.weight', 'h.3.ln_1.weight', 'h.8.attn.c_proj.weight', 'h.3.ln_1.bias', 'h.1.ln_1.bias', 'h.3.ln_2.weight', 'h.3.mlp.c_proj.bias', 'h.5.attn.c_proj.weight', 'h.6.ln_2.bias', 'h.10.ln_2.bias', 'h.3.attn.c_proj.bias', 'h.2.attn.c_proj.weight', 'h.2.mlp.c_proj.bias', 'h.1.mlp.c_proj.weight', 'h.7.mlp.c_fc.weight', 'h.7.mlp.c_proj.weight', 'h.6.ln_1.bias', 'h.5.mlp.c_fc.bias', 'h.8.ln_2.weight', 'h.9.attn.c_proj.weight', 'h.9.mlp.c_fc.bias', 'h.5.ln_1.weight', 'h.2.mlp.c_fc.bias', 'h.9.ln_1.bias', 'h.7.ln_2.bias', 'h.10.mlp.c_proj.bias', 'h.2.attn.c_attn.weight', 'h.6.mlp.c_proj.weight', 'h.8.mlp.c_proj.weight', 'h.1.attn.c_attn.weight', 'h.1.ln_2.weight', 'h.1.mlp.c_fc.bias', 'h.5.mlp.c_proj.bias', 'h.4.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.6.ln_1.weight', 'h.1.mlp.c_fc.weight', 'h.2.ln_1.bias', 'h.11.mlp.c_fc.weight', 'h.2.ln_2.bias', 'h.8.mlp.c_proj.bias', 'h.7.ln_1.weight', 'h.8.ln_2.bias', 'h.0.ln_1.weight', 'h.5.ln_2.weight', 'h.7.ln_1.bias', 'h.6.attn.c_proj.weight', 'h.1.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.7.mlp.c_proj.bias', 'h.4.ln_1.bias', 'h.8.attn.c_proj.bias', 'h.5.ln_2.bias', 'h.10.mlp.c_proj.weight', 'h.4.mlp.c_fc.bias', 'h.6.mlp.c_fc.bias', 'h.2.mlp.c_proj.weight', 'h.4.attn.c_proj.weight', 'h.6.mlp.c_proj.bias', 'h.11.ln_1.weight', 'h.6.mlp.c_fc.weight', 'h.7.attn.c_attn.weight', 'wpe.weight', 'h.0.ln_2.bias', 'h.4.ln_1.weight', 'h.9.ln_2.bias', 'h.4.mlp.c_proj.weight', 'h.2.attn.c_proj.bias', 'h.0.attn.c_proj.bias', 'h.3.attn.c_attn.weight', 'h.8.attn.c_attn.weight', 'h.11.mlp.c_proj.bias', 'h.4.mlp.c_fc.weight', 'h.9.attn.c_attn.weight', 'h.1.ln_1.weight', 'h.9.mlp.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', tokenizer=tokenizer, model='saved_nov28')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printESH slapping Carolhor\n",
      "printhorFolderfarmfarm\n",
      "print jammed Meanwhilehor cling\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generating  next word in 3 possible ways\n",
    "1. Greedy Search : chooses the best possible next word based on highest probability from 1 hypothesis\n",
    "2. Beam Search : chooses the high probability next word from n hypothesis\n",
    "3. Random Sampling : chooses random next word from possible hypothesis , however as the temperature is set high , it will\n",
    "   ignore low probability words.\n",
    "'''\n",
    "\n",
    "print(generator('print', max_length=5)[0]['generated_text'])\n",
    "print(generator('print', max_length=5,num_beams = 5)[0]['generated_text'])\n",
    "print(generator('print' , max_length=5 , do_sample=True,temperature = 0.7)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i in entail negligible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i initable cling\n",
      "for i in After affiliate\n"
     ]
    }
   ],
   "source": [
    "print(generator('for i in', max_length=5)[0]['generated_text'])\n",
    "print(generator('for i in', max_length=5,num_beams = 5)[0]['generated_text'])\n",
    "print(generator('for i in' , max_length=5 , do_sample=True,temperature = 0.7)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "extra_tokens=json.loads(open('literals.json').read())['str']\n",
    "\n",
    "extra_tokens.extend([\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "    \"<EOL>\",\n",
    "    \"<NUM_LIT>\",\n",
    "    \"<STR_LIT>\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['code_tokenizer/nov28-vocab.json', 'code_tokenizer/nov28-merges.txt']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\"./\").glob(\"**/*byCode.txt\")]\n",
    "\n",
    "paths[0:2]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=2000, min_frequency=10, special_tokens=extra_tokens)\n",
    "\n",
    "# Save files to disk\n",
    "!mkdir -p code_tokenizer\n",
    "tokenizer.save_model(\"code_tokenizer\", \"nov28\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from custom_tokenise import BPE_token\n",
    "# import os\n",
    "# # the folder 'text' contains all the files\n",
    "# paths = [str(x) for x in Path(\"./\").glob(\"**/*byCode.txt\")]\n",
    "# tokenizer = BPE_token()\n",
    "# # train the tokenizer model\n",
    "# tokenizer.bpe_train(paths)\n",
    "# # saving the tokenized data in our specified folder \n",
    "# # save_path = 'code_tokenizer'\n",
    "# # # tokenizer.save_tokenizer(save_path)\n",
    "\n",
    "# # tokenizer.save_tokenizer(\"code_tokenizer\")\n",
    "# # tokenizer = GPT2Tokenizer.from_pretrained(\"code_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained vocab size: 2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['code_tokenizer_bpe/vocab.json', 'code_tokenizer_bpe/merges.txt']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers.trainers import BpeTrainer\n",
    "import os\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import NFKC, Sequence\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.normalizer = Sequence([\n",
    "    NFKC()\n",
    "])\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "# We initialize our trainer, giving him the details about the vocabulary we want to generate\n",
    "trainer = BpeTrainer(vocab_size=2000, show_progress=True, initial_alphabet=ByteLevel.alphabet(),min_frequency=10, special_tokens=extra_tokens)\n",
    "tokenizer.train(trainer=trainer, files=paths)\n",
    "\n",
    "print(\"Trained vocab size: {}\".format(tokenizer.get_vocab_size()))\n",
    "\n",
    "!mkdir -p code_tokenizer_bpe\n",
    "tokenizer.model.save(\"code_tokenizer_bpe\", None)\n",
    "# tokenizer.save_vocabulary()\n",
    "# tokenizer.save_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from custom_tokenise import BPE_token\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "# # the folder 'text' contains all the files\n",
    "# paths = [str(x) for x in Path(\"./\").glob(\"**/*byCode.txt\")]\n",
    "# tokenizer = BPE_token()\n",
    "# # train the tokenizer model\n",
    "# tokenizer.bpe_train(paths)\n",
    "# # saving the tokenized data in our specified folder \n",
    "# save_path = 'tokenized_data'\n",
    "# tokenizer.save_tokenizer(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizer = Tokenizer.from_file(\"./code_tokenizer-vocab.json\")\n",
    "# tokenizer = ByteLevelBPETokenizer(\n",
    "#     \"code_tokenizer/nov28-vocab.json\",\n",
    "#     \"code_tokenizer/nov28-merges.txt\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file code_tokenizer_bpe/added_tokens.json. We won't load it.\n",
      "Didn't find file code_tokenizer_bpe/special_tokens_map.json. We won't load it.\n",
      "Didn't find file code_tokenizer_bpe/tokenizer_config.json. We won't load it.\n",
      "Didn't find file code_tokenizer_bpe/tokenizer.json. We won't load it.\n",
      "loading file code_tokenizer_bpe/vocab.json\n",
      "loading file code_tokenizer_bpe/merges.txt\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "loading file None\n",
      "file code_tokenizer_bpe/config.json not found\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"code_tokenizer_bpe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning </s> to the eos_token key of the tokenizer\n",
      "Assigning <s> to the bos_token key of the tokenizer\n",
      "Assigning <unk> to the unk_token key of the tokenizer\n",
      "Assigning <pad> to the pad_token key of the tokenizer\n",
      "Assigning <mask> to the mask_token key of the tokenizer\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})\n",
    "# creating the configurations from which the model can be made\n",
    "config = GPT2Config(\n",
    "  vocab_size=tokenizer.vocab_size,\n",
    "  bos_token_id=tokenizer.bos_token_id,\n",
    "  eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "# creating the model\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_string = ''\n",
    "for filename in paths:\n",
    "  with open(filename, \"r\", encoding='utf-8') as f:\n",
    "   x = f.read()\n",
    "  single_string += x + tokenizer.eos_token\n",
    "string_tokenized = tokenizer.encode(single_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[87, 397, 767]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_tokenized[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "block_size = 100\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "  examples.append(string_tokenized[i:i + block_size])\n",
    "inputs, labels = [], []\n",
    "for ex in examples:\n",
    "  inputs.append(ex[:-1])\n",
    "  labels.append(ex[1:])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# definining our loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# defining our metric which we want to observe\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# compiling the model\n",
    "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x14795f729e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x14795f729e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[12,12,99,99] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tfgp_t2lm_head_model/transformer/h_._5/attn/Softmax (defined at /home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:119) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_19220]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tfgp_t2lm_head_model/transformer/h_._5/attn/Softmax:\n tfgp_t2lm_head_model/transformer/h_._5/attn/sub_2 (defined at /home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:112)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-ed5ba231cbe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/software/venv/tf1_gpu/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[12,12,99,99] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node tfgp_t2lm_head_model/transformer/h_._5/attn/Softmax (defined at /home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:119) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_19220]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node tfgp_t2lm_head_model/transformer/h_._5/attn/Softmax:\n tfgp_t2lm_head_model/transformer/h_._5/attn/sub_2 (defined at /home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/models/gpt2/modeling_tf_gpt2.py:112)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "history = model.fit(dataset, epochs=num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorflowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
