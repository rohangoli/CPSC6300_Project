{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Dropout, LSTMCell, RNN, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import pickle\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import requests\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "import argparse\n",
    "from tokenize import tokenize, untokenize, COMMENT, STRING, NEWLINE, ENCODING, ENDMARKER, NL, INDENT, NUMBER\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = \"Project_CodeNet_Python800.tar.gz\"\n",
    "# data_url = f\"https://dax-cdn.cdn.appdomain.cloud/dax-project-codenet/1.0.0/{file_name}\"\n",
    "\n",
    "# # Download tar archive to local disk\n",
    "# with open(file_name, \"wb\") as f:\n",
    "#     f.write(requests.get(data_url).content)\n",
    "    \n",
    "# # Extract contents of archive to local disk\n",
    "# if os.path.exists(\"data\"):\n",
    "#     shutil.rmtree(\"data\")    \n",
    "# with tarfile.open(file_name) as tfile:\n",
    "#     tfile.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = glob.glob(os.path.join(os.getcwd(),\"Project_CodeNet_Python800/**/*.*\"))\n",
    "# len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lits = json.load(open(\"literals.json\"))\n",
    "\n",
    "def process_string(token, special_chars={\" \": \"U+0020\", \",\": \"U+002C\"}):\n",
    "    str_quote_options = [\"'''\", '\"\"\"', \"'\", '\"']\n",
    "    start_quote = \"\"\n",
    "    end_quote = \"\"\n",
    "    qualifier_regex = r\"^[a-z]+\"\n",
    "    qualifier_match = re.search(qualifier_regex, token)\n",
    "    # string qualifiers like 'r' for regex, 'f' for formatted string, 'b' for bytes, 'u' for unicode, etc (or combination of them)\n",
    "    qualifier = \"\" if not qualifier_match else qualifier_match[0]\n",
    "    # token string without qualifiers\n",
    "    token_string = re.sub(qualifier_regex, \"\", token)\n",
    "    # string literal without quotes\n",
    "    str_lit = token_string\n",
    "    for q in str_quote_options:\n",
    "        if token_string.startswith(q):\n",
    "            start_quote = q\n",
    "            str_lit = str_lit[len(q) :]\n",
    "            if token_string.endswith(q):\n",
    "                end_quote = q\n",
    "                str_lit = str_lit[: -len(q)]\n",
    "            break\n",
    "    # if start_quote in str_quote_options[:2]:\n",
    "    #     return \"\"\n",
    "    for sc in special_chars:\n",
    "        str_lit = str_lit.replace(sc, special_chars[sc])\n",
    "    return (\n",
    "        f\"{qualifier}{start_quote}<STR_LIT:{str_lit}>{end_quote}\"\n",
    "        if str_lit in lits['str']\n",
    "        else f\"{qualifier}{start_quote}<STR_LIT>{end_quote}\"\n",
    "    )\n",
    "\n",
    "def py_tokenize():\n",
    "    file_paths = glob.glob(os.path.join(os.getcwd(),\"Project_CodeNet_Python800/**/*.*\"))\n",
    "    wf = open(os.path.join(os.getcwd(), f\"full_corpus.txt\"), 'w')\n",
    "    local_corpus = []\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            code = open(path).read()\n",
    "            token_gen = tokenize(BytesIO(bytes(code, \"utf8\")).readline)\n",
    "            out_tokens = []\n",
    "            prev_eol = False\n",
    "            for toknum, tokval, _, _, _ in token_gen:\n",
    "                tokval = \" \".join(tokval.split())\n",
    "                if toknum == STRING:\n",
    "                    add_token = process_string(tokval)\n",
    "                    out_tokens.append(add_token)\n",
    "                    prev_eol = False\n",
    "                elif toknum == NUMBER:\n",
    "                    if tokval in lits['num']:\n",
    "                        out_tokens.append(f\"<NUM_LIT:{tokval}>\")\n",
    "                    else:\n",
    "                        out_tokens.append(f\"<NUM_LIT>\")\n",
    "                    prev_eol = False\n",
    "                elif toknum in [NEWLINE, NL]:\n",
    "                    if not prev_eol:\n",
    "                        out_tokens.append(\"<EOL>\")\n",
    "                        prev_eol = True\n",
    "                elif toknum in [COMMENT, INDENT, ENCODING, ENDMARKER] or len(tokval) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    out_tokens.append(tokval)\n",
    "                    prev_eol = False\n",
    "            if out_tokens[0] == \"<EOL>\":\n",
    "                out_tokens = out_tokens[1:]\n",
    "            if out_tokens[-1] == \"<EOL>\":\n",
    "                out_tokens = out_tokens[:-1]\n",
    "        except Exception:\n",
    "            out_tokens = []\n",
    "#         local_corpus.extend((\" \".join(out_tokens)).split('<EOL>'))\n",
    "#         out_tokens = [\"<s>\"] + out_tokens + [\"</s>\"]\n",
    "        out = \" \".join(out_tokens)\n",
    "        local_corpus.append(out)\n",
    "        wf.write(out+\"\\n\")\n",
    "    print(f\"Full Corpus is done\")\n",
    "    wf.close()\n",
    "    return local_corpus\n",
    "\n",
    "def read_corpus():\n",
    "    corpus = py_tokenize()\n",
    "    full_corpus = ''.join(corpus)\n",
    "    corpus_new = []\n",
    "    for code in corpus:\n",
    "        corpus_new.extend(code.split('<EOL>'))\n",
    "        \n",
    "    return pd.DataFrame(corpus_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('full_corpus.txt', encoding='utf8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = []\n",
    "for code in corpus:\n",
    "    text_corpus.extend(code.split(' <EOL> '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e = enumerate',\n",
       " 'n , * a = map ( int , open ( <NUM_LIT:0> ) . read ( ) . split ( ) )',\n",
       " 'd = [ <NUM_LIT:0> ]',\n",
       " 'for j , ( a , i ) in e ( sorted ( ( a , i ) for i , a in e ( a ) ) [ : : - <NUM_LIT:1> ] ) : d = [ d [ <NUM_LIT:0> ] + a * abs ( n - j - i - <NUM_LIT:1> ) ] + [ max ( d [ k ] + a * abs ( n - j + k - i - <NUM_LIT:1> ) , d [ k - <NUM_LIT:1> ] + a * abs ( i - k + <NUM_LIT:1> ) ) for k in range ( <NUM_LIT:1> , j + <NUM_LIT:1> ) ] + [ d [ j ] + a * abs ( i - j ) ]',\n",
       " 'print ( max ( d ) )\\n',\n",
       " 'N = int ( input ( ) )',\n",
       " 'A = list ( map ( int , input ( ) . split ( ) ) )',\n",
       " 'table = [ ]',\n",
       " 'for i , a in enumerate ( A ) :',\n",
       " 'table . append ( [ a , i ] )']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corpus[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent = text_corpus[0:int(0.8*len(text_corpus))]\n",
    "test_sent = text_corpus[int(0.8*len(text_corpus)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exit ( )',\n",
       " 'elif <NUM_LIT:0> < int ( S [ <NUM_LIT:2> : ] ) <= <NUM_LIT:12> :',\n",
       " \"print ( '<STR_LIT>' )\",\n",
       " 'exit ( )',\n",
       " 'else :',\n",
       " \"print ( '<STR_LIT>' )\",\n",
       " 'exit ( )\\n',\n",
       " 'S = input ( )',\n",
       " 'x = int ( S [ : <NUM_LIT:2> ] )',\n",
       " 'y = int ( S [ <NUM_LIT:2> : <NUM_LIT:4> ] )']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"train_sent.txt\", \"w\") as fp:   #Pickling\n",
    "    fp.write('\\n'.join(train_sent))\n",
    "with open(\"test_sent.txt\", \"w\") as fp:   #Pickling\n",
    "    fp.write('\\n'.join(test_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print (torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer,OpenAIGPTLMHeadModel,TextDataset,TrainingArguments,Trainer,pipeline,DataCollatorForLanguageModeling, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = OpenAIGPTTokenizer.from_pretrained(\"congcongwang/gpt2_medium_fine_tuned_coder\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type gpt2 to instantiate a model of type openai-gpt. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at congcongwang/gpt2_medium_fine_tuned_coder were not used when initializing OpenAIGPTLMHeadModel: ['transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.ln_f.weight', 'transformer.h.5.attn.masked_bias', 'transformer.ln_f.bias', 'transformer.wpe.weight', 'transformer.h.13.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.14.attn.masked_bias', 'transformer.wte.weight', 'transformer.h.18.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.1.attn.masked_bias']\n",
      "- This IS expected if you are initializing OpenAIGPTLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing OpenAIGPTLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at congcongwang/gpt2_medium_fine_tuned_coder and are newly initialized: ['transformer.positions_embed.weight', 'transformer.tokens_embed.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIGPTLMHeadModel.from_pretrained('congcongwang/gpt2_medium_fine_tuned_coder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0,1,2,3], dim=0)\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 50265, max sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary size: %d, max sequence length: %d' % (tokenizer.vocab_size, tokenizer.model_max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   242,  5457, 41949,   877,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(train_sent[0], return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'e', 'Ä =', 'Ä enumer', 'ate', '</s>']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([    0,   242,  5457, 41949,   877,     2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='train_sent.txt',\n",
    "    overwrite_cache=True,\n",
    "    block_size=19)\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='test_sent.txt',\n",
    "    overwrite_cache=True,\n",
    "    block_size=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = '/scratch1/rgoli/gpt_model', \n",
    "    overwrite_output_dir = True, \n",
    "    per_device_train_batch_size = 64, \n",
    "    per_device_eval_batch_size = 64, \n",
    "    learning_rate = 5e-4, \n",
    "    num_train_epochs = 3,\n",
    ")\n",
    "# Initializing the trainer class object that will do the training\n",
    "# here the data collator will generate the batch of size 64 of train and test data\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2425650\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 28428\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrohangoli\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rohangoli/huggingface/runs/9rs8bydg\" target=\"_blank\">/scratch1/rgoli/gpt_model</a></strong> to <a href=\"https://wandb.ai/rohangoli/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28428' max='28428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28428/28428 3:05:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.439200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.293100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>4.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>4.219500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>4.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>4.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>4.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>4.182500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>4.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>4.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>4.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>4.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>4.153300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>4.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>4.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>4.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>4.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>4.157300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>4.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>4.150100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>4.146400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>4.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>4.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>4.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>4.146700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>4.143100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>4.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>4.143400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>4.142800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>4.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>4.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>4.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>4.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>4.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>4.142000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>4.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>4.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>4.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>4.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>4.137300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>4.137900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>4.139800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>4.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>4.140600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-1000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-1000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-1000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-1500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-1500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-1500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-2000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-2000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-2000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-2500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-2500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-2500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-3000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-3000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-3000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-3500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-3500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-3500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-4000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-4000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-4000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-4500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-4500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-4500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-5000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-5000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-5000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-5500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-5500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-5500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-6000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-6000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-6000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-6500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-6500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-6500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-7000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-7000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-7000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-7500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-7500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-7500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-8000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-8000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-8000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-8500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-8500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-8500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-9000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-9000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-9000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-9500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-9500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-9500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-10000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-10000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-10000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-10500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-10500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-10500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-11000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-11000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-11000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-11500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-11500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-11500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-12000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-12000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-12000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-12500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-12500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-12500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-13000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-13000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-13000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-13500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-13500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-13500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-14000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-14000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-14000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-14500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-14500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-14500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-15000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-15000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-15000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-15500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-15500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-15500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-16000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-16000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-16000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-16500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-16500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-16500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-17000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-17000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-17000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-17500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-17500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-17500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-18000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-18000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-18000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-18500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-18500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-18500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-19000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-19000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-19000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-19500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-19500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-19500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-20000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-20000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-20000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-20500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-20500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-20500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-21000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-21000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-21000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-21500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-21500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-21500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-22000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-22000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-22000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-22500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-22500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-22500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-23000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-23000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-23000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-23500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-23500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-23500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-24000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-24000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-24000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-24500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-24500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-24500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-25000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-25000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-25000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-25500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-25500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-25500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-26000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-26000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-26000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-26500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-26500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-26500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-27000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-27000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-27000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-27500\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-27500/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-27500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to /scratch1/rgoli/gpt_model/checkpoint-28000\n",
      "Configuration saved in /scratch1/rgoli/gpt_model/checkpoint-28000/config.json\n",
      "Model weights saved in /scratch1/rgoli/gpt_model/checkpoint-28000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=28428, training_loss=4.174119551339128, metrics={'train_runtime': 11140.435, 'train_samples_per_second': 653.202, 'train_steps_per_second': 2.552, 'total_flos': 0.0, 'train_loss': 4.174119551339128, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model for 3 epochs\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved\n",
      "Configuration saved in ./saved/config.json\n",
      "Model weights saved in ./saved/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint save for each 3000 steps instead of 500 steps\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./saved',\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     logging_steps=3000,\n",
    "#     save_steps=3000,\n",
    "#     save_total_limit=2,\n",
    "#     seed=1,\n",
    "#     fp16=True\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# trainer.save_model('./saved')\n",
    "\n",
    "## For more details https://discuss.huggingface.co/t/loading-model-from-checkpoint-after-error-in-training/758/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multi-Device GPU training on PyTorch\n",
    "# https://pythonmana.com/2021/07/20210707185846843x.html\n",
    "\n",
    "#  Load model\n",
    "\n",
    "# model = nn.DataParallel(model)\n",
    "# model = model.cuda()\n",
    "\n",
    "# Directly, of course device_ids It's fine too ï¼š\n",
    "\n",
    "# net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n",
    "# model = model.cuda()\n",
    "\n",
    "# Load data\n",
    "\n",
    "# inputs = inputs.cuda()\n",
    "# labels = labels.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 603781\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2359' max='2359' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2359/2359 04:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.8302130699157715,\n",
       " 'eval_runtime': 296.9818,\n",
       " 'eval_samples_per_second': 2033.057,\n",
       " 'eval_steps_per_second': 7.943,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating on Test data\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file saved/config.json\n",
      "Model config OpenAIGPTConfig {\n",
      "  \"_name_or_path\": \"congcongwang/gpt2_medium_fine_tuned_coder\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "loading configuration file saved/config.json\n",
      "Model config OpenAIGPTConfig {\n",
      "  \"_name_or_path\": \"congcongwang/gpt2_medium_fine_tuned_coder\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"afn\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"OpenAIGPTLMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"openai-gpt\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"vocab_size\": 50260\n",
      "}\n",
      "\n",
      "loading weights file saved/pytorch_model.bin\n",
      "Some weights of the model checkpoint at saved were not used when initializing OpenAIGPTLMHeadModel: ['module.transformer.h.7.mlp.c_fc.weight', 'module.transformer.h.23.mlp.c_fc.bias', 'module.transformer.h.15.ln_1.weight', 'module.transformer.h.13.ln_1.bias', 'module.transformer.h.16.ln_2.bias', 'module.transformer.tokens_embed.weight', 'module.transformer.h.6.attn.c_proj.weight', 'module.transformer.h.17.attn.c_proj.weight', 'module.transformer.h.21.ln_1.weight', 'module.transformer.h.19.ln_2.bias', 'module.transformer.h.19.attn.c_attn.bias', 'module.transformer.h.9.ln_1.bias', 'module.transformer.h.3.ln_2.bias', 'module.transformer.h.0.ln_1.bias', 'module.transformer.h.13.attn.c_attn.weight', 'module.transformer.h.15.attn.c_proj.weight', 'module.transformer.h.16.mlp.c_proj.weight', 'module.transformer.h.23.mlp.c_fc.weight', 'module.transformer.h.8.ln_1.bias', 'module.transformer.h.9.mlp.c_proj.weight', 'module.transformer.h.7.mlp.c_fc.bias', 'module.transformer.h.14.attn.bias', 'module.transformer.h.8.ln_1.weight', 'module.transformer.h.17.mlp.c_fc.bias', 'module.transformer.h.11.attn.bias', 'module.transformer.h.17.ln_1.weight', 'module.transformer.h.13.attn.bias', 'module.transformer.h.17.mlp.c_proj.weight', 'module.transformer.h.20.attn.c_proj.bias', 'module.transformer.h.17.mlp.c_proj.bias', 'module.transformer.h.8.attn.c_proj.bias', 'module.transformer.h.11.ln_2.bias', 'module.transformer.h.23.ln_2.weight', 'module.transformer.h.0.ln_2.bias', 'module.transformer.h.16.mlp.c_proj.bias', 'module.transformer.h.4.mlp.c_proj.bias', 'module.transformer.h.6.ln_1.bias', 'module.transformer.h.5.mlp.c_fc.bias', 'module.transformer.h.6.mlp.c_proj.bias', 'module.transformer.h.2.attn.c_proj.bias', 'module.transformer.h.3.attn.bias', 'module.transformer.h.10.ln_1.weight', 'module.transformer.h.4.mlp.c_proj.weight', 'module.transformer.h.1.mlp.c_fc.bias', 'module.transformer.h.15.attn.bias', 'module.transformer.h.23.ln_2.bias', 'module.transformer.h.10.attn.c_proj.bias', 'module.transformer.h.10.mlp.c_proj.weight', 'module.transformer.h.23.attn.c_proj.bias', 'module.transformer.h.11.ln_1.bias', 'module.transformer.h.9.mlp.c_fc.weight', 'module.transformer.h.23.mlp.c_proj.bias', 'module.transformer.h.23.mlp.c_proj.weight', 'module.transformer.h.12.mlp.c_proj.weight', 'module.transformer.h.7.attn.c_attn.weight', 'module.transformer.h.2.attn.c_proj.weight', 'module.transformer.h.11.mlp.c_proj.bias', 'module.transformer.h.0.ln_2.weight', 'module.transformer.h.4.mlp.c_fc.bias', 'module.transformer.h.6.attn.c_attn.bias', 'module.transformer.h.11.attn.c_proj.weight', 'module.transformer.h.22.attn.c_proj.bias', 'module.transformer.h.7.attn.c_proj.bias', 'module.transformer.h.15.mlp.c_proj.weight', 'module.transformer.h.13.attn.c_proj.weight', 'module.transformer.h.1.mlp.c_fc.weight', 'module.transformer.h.7.attn.bias', 'module.transformer.h.20.ln_2.weight', 'module.transformer.h.19.ln_1.weight', 'module.transformer.h.18.attn.c_proj.bias', 'module.transformer.h.12.attn.c_proj.bias', 'module.transformer.h.20.attn.bias', 'module.transformer.h.1.attn.c_proj.weight', 'module.transformer.h.3.mlp.c_fc.bias', 'module.transformer.h.8.attn.c_attn.weight', 'module.transformer.h.22.ln_1.weight', 'module.transformer.h.15.attn.c_attn.weight', 'module.transformer.h.17.attn.c_proj.bias', 'module.transformer.h.19.mlp.c_fc.bias', 'module.transformer.h.13.ln_2.bias', 'module.transformer.h.10.ln_2.weight', 'module.transformer.h.13.ln_1.weight', 'module.transformer.h.20.mlp.c_fc.bias', 'module.transformer.h.8.mlp.c_proj.bias', 'module.transformer.h.18.mlp.c_proj.bias', 'module.transformer.h.6.attn.c_attn.weight', 'module.transformer.h.5.attn.c_attn.bias', 'module.transformer.h.17.ln_2.weight', 'module.transformer.h.22.ln_2.weight', 'module.transformer.h.16.attn.c_attn.bias', 'module.transformer.h.21.attn.bias', 'module.transformer.h.23.ln_1.weight', 'module.transformer.h.15.ln_1.bias', 'module.transformer.h.0.attn.bias', 'module.transformer.h.21.attn.c_attn.bias', 'module.transformer.h.22.attn.c_proj.weight', 'module.transformer.h.12.mlp.c_fc.weight', 'module.transformer.h.19.ln_1.bias', 'module.transformer.h.22.attn.bias', 'module.transformer.h.12.mlp.c_proj.bias', 'module.transformer.h.4.attn.c_attn.weight', 'module.transformer.h.13.mlp.c_proj.bias', 'module.transformer.h.2.ln_1.weight', 'module.transformer.h.4.ln_1.bias', 'module.transformer.h.13.attn.c_attn.bias', 'module.transformer.h.15.mlp.c_proj.bias', 'module.transformer.h.8.attn.bias', 'module.transformer.h.22.ln_1.bias', 'module.transformer.h.10.attn.c_proj.weight', 'module.transformer.h.14.mlp.c_fc.weight', 'module.transformer.h.19.attn.bias', 'module.transformer.h.16.attn.c_proj.bias', 'module.transformer.h.22.mlp.c_proj.bias', 'module.transformer.h.2.mlp.c_proj.bias', 'module.transformer.h.6.mlp.c_fc.bias', 'module.transformer.h.14.ln_2.bias', 'module.transformer.h.11.mlp.c_fc.weight', 'module.transformer.h.8.ln_2.weight', 'module.transformer.h.3.ln_2.weight', 'module.lm_head.weight', 'module.transformer.h.4.attn.bias', 'module.transformer.h.20.attn.c_attn.weight', 'module.transformer.h.11.ln_2.weight', 'module.transformer.h.4.attn.c_attn.bias', 'module.transformer.h.18.attn.c_attn.bias', 'module.transformer.h.7.mlp.c_proj.weight', 'module.transformer.h.18.mlp.c_proj.weight', 'module.transformer.h.12.attn.bias', 'module.transformer.h.16.ln_1.bias', 'module.transformer.h.12.ln_2.weight', 'module.transformer.h.18.mlp.c_fc.weight', 'module.transformer.h.7.ln_1.bias', 'module.transformer.h.10.ln_1.bias', 'module.transformer.h.5.ln_1.weight', 'module.transformer.h.20.mlp.c_proj.bias', 'module.transformer.h.0.attn.c_proj.bias', 'module.transformer.h.13.attn.c_proj.bias', 'module.transformer.h.13.mlp.c_proj.weight', 'module.transformer.h.6.attn.bias', 'module.transformer.h.18.attn.c_proj.weight', 'module.transformer.h.9.attn.c_attn.bias', 'module.transformer.h.4.ln_2.weight', 'module.transformer.h.9.mlp.c_fc.bias', 'module.transformer.h.1.attn.c_attn.weight', 'module.transformer.h.2.ln_2.bias', 'module.transformer.h.22.attn.c_attn.bias', 'module.transformer.h.21.ln_2.weight', 'module.transformer.h.7.attn.c_attn.bias', 'module.transformer.h.17.attn.c_attn.bias', 'module.transformer.h.1.ln_1.weight', 'module.transformer.h.5.mlp.c_proj.bias', 'module.transformer.h.21.ln_1.bias', 'module.transformer.h.0.mlp.c_proj.weight', 'module.transformer.h.8.mlp.c_fc.bias', 'module.transformer.h.3.mlp.c_fc.weight', 'module.transformer.h.11.attn.c_attn.bias', 'module.transformer.h.17.ln_1.bias', 'module.transformer.h.8.mlp.c_fc.weight', 'module.transformer.h.20.ln_2.bias', 'module.transformer.h.23.attn.c_attn.weight', 'module.transformer.h.5.ln_1.bias', 'module.transformer.h.2.mlp.c_proj.weight', 'module.transformer.h.7.ln_1.weight', 'module.transformer.h.1.ln_1.bias', 'module.transformer.h.5.mlp.c_proj.weight', 'module.transformer.h.4.ln_2.bias', 'module.transformer.h.2.attn.c_attn.weight', 'module.transformer.h.21.ln_2.bias', 'module.transformer.h.12.ln_1.weight', 'module.transformer.h.8.attn.c_attn.bias', 'module.transformer.h.10.attn.bias', 'module.transformer.h.20.attn.c_attn.bias', 'module.transformer.h.22.attn.c_attn.weight', 'module.transformer.h.2.mlp.c_fc.weight', 'module.transformer.h.9.attn.c_proj.bias', 'module.transformer.h.5.attn.c_attn.weight', 'module.transformer.h.4.attn.c_proj.bias', 'module.transformer.h.19.attn.c_proj.bias', 'module.transformer.h.6.mlp.c_proj.weight', 'module.transformer.h.1.mlp.c_proj.bias', 'module.transformer.h.19.mlp.c_fc.weight', 'module.transformer.h.0.attn.c_attn.bias', 'module.transformer.h.14.mlp.c_proj.weight', 'module.transformer.h.12.attn.c_attn.bias', 'module.transformer.h.16.ln_1.weight', 'module.transformer.h.21.mlp.c_proj.weight', 'module.transformer.h.0.mlp.c_fc.weight', 'module.transformer.h.21.mlp.c_fc.weight', 'module.transformer.h.21.attn.c_proj.weight', 'module.transformer.h.21.mlp.c_fc.bias', 'module.transformer.h.17.ln_2.bias', 'module.transformer.h.5.attn.bias', 'module.transformer.h.23.attn.c_proj.weight', 'module.transformer.h.19.mlp.c_proj.bias', 'module.transformer.h.10.mlp.c_proj.bias', 'module.transformer.h.2.attn.c_attn.bias', 'module.transformer.h.5.mlp.c_fc.weight', 'module.transformer.h.14.ln_1.bias', 'module.transformer.h.3.ln_1.weight', 'module.transformer.h.19.attn.c_proj.weight', 'module.transformer.h.9.ln_1.weight', 'module.transformer.h.1.attn.bias', 'module.transformer.h.12.ln_1.bias', 'module.transformer.h.17.attn.bias', 'module.transformer.h.6.mlp.c_fc.weight', 'module.transformer.h.6.ln_2.weight', 'module.transformer.h.9.ln_2.weight', 'module.transformer.h.23.attn.bias', 'module.transformer.h.14.mlp.c_proj.bias', 'module.transformer.h.19.ln_2.weight', 'module.transformer.h.10.mlp.c_fc.weight', 'module.transformer.h.11.attn.c_attn.weight', 'module.transformer.h.17.attn.c_attn.weight', 'module.transformer.h.18.attn.c_attn.weight', 'module.transformer.h.16.attn.c_proj.weight', 'module.transformer.h.14.ln_2.weight', 'module.transformer.h.9.attn.c_attn.weight', 'module.transformer.h.21.attn.c_attn.weight', 'module.transformer.h.18.ln_1.weight', 'module.transformer.h.6.attn.c_proj.bias', 'module.transformer.h.21.attn.c_proj.bias', 'module.transformer.h.9.attn.c_proj.weight', 'module.transformer.h.10.ln_2.bias', 'module.transformer.h.21.mlp.c_proj.bias', 'module.transformer.h.1.ln_2.bias', 'module.transformer.h.2.ln_1.bias', 'module.transformer.h.18.mlp.c_fc.bias', 'module.transformer.h.3.attn.c_proj.bias', 'module.transformer.h.11.ln_1.weight', 'module.transformer.h.16.attn.bias', 'module.transformer.h.4.ln_1.weight', 'module.transformer.h.12.attn.c_proj.weight', 'module.transformer.h.3.attn.c_attn.weight', 'module.transformer.h.11.mlp.c_proj.weight', 'module.transformer.h.20.ln_1.bias', 'module.transformer.h.1.mlp.c_proj.weight', 'module.transformer.h.14.ln_1.weight', 'module.transformer.h.1.attn.c_proj.bias', 'module.transformer.h.0.attn.c_proj.weight', 'module.transformer.h.14.attn.c_proj.bias', 'module.transformer.h.0.attn.c_attn.weight', 'module.transformer.h.3.attn.c_proj.weight', 'module.transformer.h.14.mlp.c_fc.bias', 'module.transformer.h.22.mlp.c_fc.bias', 'module.transformer.h.14.attn.c_proj.weight', 'module.transformer.h.22.mlp.c_proj.weight', 'module.transformer.h.7.ln_2.weight', 'module.transformer.h.12.mlp.c_fc.bias', 'module.transformer.h.19.attn.c_attn.weight', 'module.transformer.h.6.ln_1.weight', 'module.transformer.h.1.ln_2.weight', 'module.transformer.h.20.attn.c_proj.weight', 'module.transformer.h.16.attn.c_attn.weight', 'module.transformer.h.9.mlp.c_proj.bias', 'module.transformer.h.15.attn.c_attn.bias', 'module.transformer.h.3.ln_1.bias', 'module.transformer.h.20.mlp.c_proj.weight', 'module.transformer.h.7.attn.c_proj.weight', 'module.transformer.h.16.mlp.c_fc.weight', 'module.transformer.h.5.attn.c_proj.weight', 'module.transformer.h.12.attn.c_attn.weight', 'module.transformer.h.18.ln_1.bias', 'module.transformer.h.0.mlp.c_proj.bias', 'module.transformer.h.3.attn.c_attn.bias', 'module.transformer.h.5.attn.c_proj.bias', 'module.transformer.h.13.ln_2.weight', 'module.transformer.h.16.mlp.c_fc.bias', 'module.transformer.h.3.mlp.c_proj.weight', 'module.transformer.h.9.ln_2.bias', 'module.transformer.h.11.mlp.c_fc.bias', 'module.transformer.h.2.ln_2.weight', 'module.transformer.h.5.ln_2.bias', 'module.transformer.h.10.attn.c_attn.weight', 'module.transformer.h.10.mlp.c_fc.bias', 'module.transformer.h.8.attn.c_proj.weight', 'module.transformer.h.12.ln_2.bias', 'module.transformer.h.20.mlp.c_fc.weight', 'module.transformer.h.23.attn.c_attn.bias', 'module.transformer.h.5.ln_2.weight', 'module.transformer.h.10.attn.c_attn.bias', 'module.transformer.h.6.ln_2.bias', 'module.transformer.h.16.ln_2.weight', 'module.transformer.h.9.attn.bias', 'module.transformer.h.15.mlp.c_fc.bias', 'module.transformer.h.15.ln_2.bias', 'module.transformer.h.19.mlp.c_proj.weight', 'module.transformer.h.4.attn.c_proj.weight', 'module.transformer.h.7.mlp.c_proj.bias', 'module.transformer.h.2.mlp.c_fc.bias', 'module.transformer.h.0.mlp.c_fc.bias', 'module.transformer.h.15.ln_2.weight', 'module.transformer.positions_embed.weight', 'module.transformer.h.11.attn.c_proj.bias', 'module.transformer.h.7.ln_2.bias', 'module.transformer.h.22.mlp.c_fc.weight', 'module.transformer.h.3.mlp.c_proj.bias', 'module.transformer.h.8.mlp.c_proj.weight', 'module.transformer.h.20.ln_1.weight', 'module.transformer.h.4.mlp.c_fc.weight', 'module.transformer.h.15.attn.c_proj.bias', 'module.transformer.h.18.ln_2.bias', 'module.transformer.h.18.attn.bias', 'module.transformer.h.2.attn.bias', 'module.transformer.h.23.ln_1.bias', 'module.transformer.h.17.mlp.c_fc.weight', 'module.transformer.h.14.attn.c_attn.bias', 'module.transformer.position_ids', 'module.transformer.h.0.ln_1.weight', 'module.transformer.h.18.ln_2.weight', 'module.transformer.h.1.attn.c_attn.bias', 'module.transformer.h.13.mlp.c_fc.bias', 'module.transformer.h.13.mlp.c_fc.weight', 'module.transformer.h.8.ln_2.bias', 'module.transformer.h.14.attn.c_attn.weight', 'module.transformer.h.15.mlp.c_fc.weight', 'module.transformer.h.22.ln_2.bias']\n",
      "- This IS expected if you are initializing OpenAIGPTLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing OpenAIGPTLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at saved and are newly initialized: ['h.20.mlp.c_proj.weight', 'h.16.mlp.c_proj.bias', 'h.6.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.19.attn.bias', 'h.1.attn.bias', 'h.5.attn.c_attn.weight', 'h.23.attn.c_proj.weight', 'h.3.attn.c_attn.bias', 'h.17.attn.c_proj.bias', 'h.19.ln_1.bias', 'h.2.mlp.c_fc.weight', 'h.7.attn.c_proj.bias', 'h.14.ln_2.weight', 'h.8.attn.bias', 'h.21.mlp.c_fc.weight', 'h.12.ln_1.weight', 'h.4.attn.bias', 'h.3.ln_1.bias', 'h.10.ln_2.bias', 'h.10.attn.c_proj.weight', 'h.7.attn.bias', 'h.12.ln_1.bias', 'h.21.ln_2.weight', 'h.21.attn.c_proj.weight', 'h.10.mlp.c_fc.weight', 'h.7.mlp.c_proj.weight', 'h.8.mlp.c_fc.bias', 'h.1.attn.c_proj.weight', 'h.14.mlp.c_fc.bias', 'h.6.ln_1.weight', 'h.23.attn.c_attn.weight', 'h.5.mlp.c_proj.bias', 'h.16.mlp.c_proj.weight', 'h.2.attn.c_attn.weight', 'h.3.ln_2.weight', 'h.22.ln_2.bias', 'h.15.mlp.c_fc.weight', 'h.8.ln_1.weight', 'h.21.attn.c_attn.weight', 'h.16.ln_1.bias', 'h.11.ln_1.weight', 'h.23.ln_2.weight', 'h.23.mlp.c_proj.bias', 'h.8.attn.c_proj.bias', 'h.5.attn.bias', 'h.20.attn.bias', 'h.1.ln_2.bias', 'h.14.ln_2.bias', 'h.12.ln_2.bias', 'h.2.attn.c_proj.weight', 'h.19.attn.c_proj.bias', 'h.20.ln_2.weight', 'h.5.ln_1.bias', 'h.16.attn.c_attn.weight', 'h.14.ln_1.bias', 'h.1.ln_1.bias', 'h.23.mlp.c_fc.weight', 'h.13.attn.c_proj.weight', 'h.20.mlp.c_fc.bias', 'h.7.mlp.c_fc.bias', 'h.1.attn.c_proj.bias', 'h.0.attn.bias', 'h.19.attn.c_attn.weight', 'h.6.attn.c_proj.weight', 'h.12.attn.c_proj.weight', 'h.15.attn.c_proj.weight', 'h.22.attn.bias', 'h.4.ln_2.bias', 'h.21.attn.c_proj.bias', 'h.4.attn.c_attn.bias', 'h.13.attn.c_attn.bias', 'h.14.attn.c_attn.weight', 'h.9.mlp.c_proj.weight', 'h.0.ln_2.bias', 'h.5.ln_2.weight', 'h.8.attn.c_attn.weight', 'h.18.ln_2.weight', 'h.11.mlp.c_fc.bias', 'h.10.attn.c_attn.bias', 'h.16.ln_2.bias', 'h.13.ln_1.bias', 'h.1.attn.c_attn.weight', 'h.10.mlp.c_proj.weight', 'h.0.mlp.c_proj.bias', 'h.14.attn.c_proj.weight', 'h.10.ln_2.weight', 'h.11.attn.c_proj.bias', 'h.16.attn.c_proj.weight', 'h.13.attn.bias', 'h.23.ln_1.bias', 'h.18.mlp.c_fc.bias', 'h.6.attn.c_proj.bias', 'h.21.mlp.c_proj.bias', 'h.9.mlp.c_fc.bias', 'h.17.mlp.c_fc.bias', 'h.15.attn.c_proj.bias', 'h.2.ln_2.weight', 'h.20.ln_1.bias', 'h.10.ln_1.bias', 'h.4.attn.c_proj.weight', 'h.4.mlp.c_proj.bias', 'h.3.attn.bias', 'h.4.attn.c_proj.bias', 'h.12.attn.c_attn.bias', 'h.16.ln_1.weight', 'h.5.mlp.c_fc.bias', 'h.14.attn.c_proj.bias', 'h.6.attn.c_attn.bias', 'h.17.attn.c_attn.weight', 'h.1.mlp.c_fc.bias', 'h.9.ln_2.bias', 'h.19.ln_2.weight', 'h.3.mlp.c_proj.weight', 'h.8.attn.c_attn.bias', 'h.11.ln_2.bias', 'h.19.ln_1.weight', 'h.23.mlp.c_fc.bias', 'h.12.mlp.c_proj.bias', 'h.6.mlp.c_fc.weight', 'h.3.mlp.c_proj.bias', 'h.8.attn.c_proj.weight', 'h.0.mlp.c_proj.weight', 'h.18.mlp.c_fc.weight', 'h.15.attn.bias', 'h.7.ln_1.bias', 'h.8.mlp.c_proj.weight', 'h.8.ln_1.bias', 'h.8.ln_2.weight', 'h.8.mlp.c_proj.bias', 'h.11.mlp.c_fc.weight', 'h.17.ln_1.bias', 'h.10.attn.c_attn.weight', 'tokens_embed.weight', 'h.15.mlp.c_proj.bias', 'h.3.ln_1.weight', 'h.10.mlp.c_proj.bias', 'h.0.ln_2.weight', 'h.17.ln_2.weight', 'h.12.attn.c_attn.weight', 'h.22.ln_1.bias', 'h.17.attn.c_proj.weight', 'h.5.mlp.c_fc.weight', 'h.1.attn.c_attn.bias', 'h.23.ln_1.weight', 'h.13.ln_2.weight', 'h.8.ln_2.bias', 'h.16.mlp.c_fc.weight', 'h.3.attn.c_proj.bias', 'h.12.attn.c_proj.bias', 'h.11.ln_1.bias', 'h.12.ln_2.weight', 'h.0.ln_1.bias', 'h.0.attn.c_proj.weight', 'h.18.attn.c_attn.bias', 'h.20.ln_1.weight', 'h.2.mlp.c_fc.bias', 'h.2.attn.c_attn.bias', 'h.5.ln_2.bias', 'h.12.mlp.c_fc.bias', 'h.14.mlp.c_fc.weight', 'h.9.ln_1.bias', 'h.15.ln_2.weight', 'h.9.attn.c_proj.bias', 'h.15.mlp.c_fc.bias', 'h.11.mlp.c_proj.weight', 'h.1.ln_1.weight', 'h.1.ln_2.weight', 'h.4.attn.c_attn.weight', 'h.16.ln_2.weight', 'h.10.ln_1.weight', 'h.12.attn.bias', 'h.11.attn.c_attn.weight', 'h.18.mlp.c_proj.bias', 'h.4.ln_1.weight', 'h.19.mlp.c_proj.bias', 'h.20.ln_2.bias', 'h.18.attn.c_attn.weight', 'h.21.attn.bias', 'h.5.attn.c_proj.bias', 'h.4.ln_1.bias', 'h.5.ln_1.weight', 'h.15.mlp.c_proj.weight', 'h.22.ln_1.weight', 'h.22.mlp.c_fc.weight', 'h.9.mlp.c_fc.weight', 'h.13.mlp.c_proj.bias', 'h.19.attn.c_proj.weight', 'h.22.ln_2.weight', 'h.21.ln_2.bias', 'h.23.mlp.c_proj.weight', 'h.16.attn.c_attn.bias', 'h.22.attn.c_proj.weight', 'h.20.attn.c_proj.bias', 'h.9.ln_1.weight', 'h.17.ln_1.weight', 'h.2.ln_2.bias', 'h.19.mlp.c_proj.weight', 'h.2.mlp.c_proj.weight', 'h.7.attn.c_attn.weight', 'h.9.attn.c_attn.weight', 'h.7.mlp.c_proj.bias', 'h.2.attn.c_proj.bias', 'h.14.attn.bias', 'h.15.attn.c_attn.weight', 'positions_embed.weight', 'h.22.mlp.c_proj.bias', 'h.17.mlp.c_fc.weight', 'h.10.attn.c_proj.bias', 'h.18.attn.bias', 'h.6.mlp.c_proj.weight', 'h.18.ln_2.bias', 'h.16.attn.bias', 'h.13.ln_2.bias', 'h.9.attn.bias', 'h.17.attn.bias', 'h.17.mlp.c_proj.weight', 'h.21.ln_1.weight', 'h.22.attn.c_attn.bias', 'h.0.attn.c_attn.bias', 'h.17.mlp.c_proj.bias', 'h.7.attn.c_attn.bias', 'h.21.ln_1.bias', 'h.13.attn.c_proj.bias', 'h.17.attn.c_attn.bias', 'h.0.mlp.c_fc.weight', 'h.23.attn.c_proj.bias', 'h.3.mlp.c_fc.bias', 'h.2.ln_1.weight', 'h.13.mlp.c_fc.bias', 'h.11.attn.bias', 'h.9.ln_2.weight', 'h.9.attn.c_proj.weight', 'h.20.mlp.c_proj.bias', 'h.5.attn.c_proj.weight', 'h.11.mlp.c_proj.bias', 'h.15.ln_2.bias', 'h.10.mlp.c_fc.bias', 'h.6.ln_2.weight', 'h.4.mlp.c_proj.weight', 'h.5.attn.c_attn.bias', 'h.22.mlp.c_fc.bias', 'h.13.ln_1.weight', 'h.21.attn.c_attn.bias', 'h.18.attn.c_proj.weight', 'h.9.attn.c_attn.bias', 'h.5.mlp.c_proj.weight', 'h.22.attn.c_proj.bias', 'h.14.attn.c_attn.bias', 'h.2.ln_1.bias', 'h.17.ln_2.bias', 'h.3.mlp.c_fc.weight', 'h.6.ln_2.bias', 'h.19.attn.c_attn.bias', 'h.15.ln_1.bias', 'h.20.mlp.c_fc.weight', 'h.14.ln_1.weight', 'h.8.mlp.c_fc.weight', 'h.13.attn.c_attn.weight', 'h.3.attn.c_attn.weight', 'h.6.attn.bias', 'lm_head.weight', 'h.6.ln_1.bias', 'h.20.attn.c_attn.weight', 'h.12.mlp.c_proj.weight', 'h.18.attn.c_proj.bias', 'h.20.attn.c_proj.weight', 'h.19.mlp.c_fc.bias', 'h.21.mlp.c_fc.bias', 'h.15.ln_1.weight', 'h.4.mlp.c_fc.weight', 'h.22.attn.c_attn.weight', 'h.7.ln_2.bias', 'h.4.mlp.c_fc.bias', 'h.3.ln_2.bias', 'h.11.ln_2.weight', 'h.23.attn.c_attn.bias', 'h.10.attn.bias', 'h.14.mlp.c_proj.bias', 'h.9.mlp.c_proj.bias', 'h.1.mlp.c_proj.weight', 'h.23.attn.bias', 'h.4.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.13.mlp.c_proj.weight', 'h.1.mlp.c_proj.bias', 'h.18.ln_1.weight', 'h.2.attn.bias', 'h.6.mlp.c_fc.bias', 'h.19.mlp.c_fc.weight', 'h.2.mlp.c_proj.bias', 'h.7.mlp.c_fc.weight', 'h.15.attn.c_attn.bias', 'h.23.ln_2.bias', 'h.6.mlp.c_proj.bias', 'h.12.mlp.c_fc.weight', 'h.7.ln_1.weight', 'h.13.mlp.c_fc.weight', 'h.7.ln_2.weight', 'h.0.attn.c_attn.weight', 'h.19.ln_2.bias', 'h.16.mlp.c_fc.bias', 'h.14.mlp.c_proj.weight', 'h.21.mlp.c_proj.weight', 'h.11.attn.c_proj.weight', 'h.20.attn.c_attn.bias', 'h.16.attn.c_proj.bias', 'h.3.attn.c_proj.weight', 'h.18.mlp.c_proj.weight', 'h.22.mlp.c_proj.weight', 'h.18.ln_1.bias', 'h.0.ln_1.weight', 'h.1.mlp.c_fc.weight', 'h.11.attn.c_attn.bias', 'h.7.attn.c_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/microsoft/codebert-base-mlm/resolve/main/config.json from cache at /home/rgoli/.cache/huggingface/transformers/5d9019d18b777cb509968283800fb631e8ea149593a98d7a4812126e5684c339.a231bc6d2804e29cfa7a5da0d6baa439072f7702dcc835c72313070f9c9bde1b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/microsoft/codebert-base-mlm/resolve/main/vocab.json from cache at /home/rgoli/.cache/huggingface/transformers/8d008cc74faa02ae1254d3fa84ac927330de3a642874ffbedc7f544f9b26a61f.647b4548b6d9ea817e82e7a9231a320231a1c9ea24053cc9e758f3fe68216f05\n",
      "loading file https://huggingface.co/microsoft/codebert-base-mlm/resolve/main/merges.txt from cache at /home/rgoli/.cache/huggingface/transformers/95f120e954ac7f008090da9be0956a6d1dda7c258a4519f169ceba8773c6e969.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/microsoft/codebert-base-mlm/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/codebert-base-mlm/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/microsoft/codebert-base-mlm/resolve/main/special_tokens_map.json from cache at /home/rgoli/.cache/huggingface/transformers/6b6d54aefb63b9d58f063d74c065c9b46f06a8d4021859f4a1334aa6779e2528.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n",
      "loading file https://huggingface.co/microsoft/codebert-base-mlm/resolve/main/tokenizer_config.json from cache at /home/rgoli/.cache/huggingface/transformers/312865c861a2237d3bcce13fa58d2bce84bb5b3133f9e8fbb97172b94c4d1823.024cc07195c0ba0b51d4f80061c6115996ff26233f3d04788855b23cdf13fbd5\n",
      "loading configuration file https://huggingface.co/microsoft/codebert-base-mlm/resolve/main/config.json from cache at /home/rgoli/.cache/huggingface/transformers/5d9019d18b777cb509968283800fb631e8ea149593a98d7a4812126e5684c339.a231bc6d2804e29cfa7a5da0d6baa439072f7702dcc835c72313070f9c9bde1b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/microsoft/codebert-base-mlm/resolve/main/config.json from cache at /home/rgoli/.cache/huggingface/transformers/5d9019d18b777cb509968283800fb631e8ea149593a98d7a4812126e5684c339.a231bc6d2804e29cfa7a5da0d6baa439072f7702dcc835c72313070f9c9bde1b\n",
      "Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', tokenizer='microsoft/codebert-base-mlm', model='saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print actresses Mueller nerv Mueller\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print Maiden IntegNormally aloud\n",
      "print Previously rising aloud assured\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "generating  next word in 3 possible ways\n",
    "1. Greedy Search : chooses the best possible next word based on highest probability from 1 hypothesis\n",
    "2. Beam Search : chooses the high probability next word from n hypothesis\n",
    "3. Random Sampling : chooses random next word from possible hypothesis , however as the temperature is set high , it will\n",
    "   ignore low probability words.\n",
    "'''\n",
    "\n",
    "print(generator('print', max_length=5)[0]['generated_text'])\n",
    "print(generator('print', max_length=5,num_beams = 5)[0]['generated_text'])\n",
    "print(generator('print' , max_length=5 , do_sample=True,temperature = 0.7)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generator('print', max_length=5)[0]['generated_text'])\n",
    "print(generator('print', max_length=5,num_beams = 5)[0]['generated_text'])\n",
    "print(generator('print' , max_length=5 , do_sample=True,temperature = 0.7)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i in agg regards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for i in202 plagued\n",
      "for i in plagued Jonathan\n"
     ]
    }
   ],
   "source": [
    "print(generator('for i in', max_length=5)[0]['generated_text'])\n",
    "print(generator('for i in', max_length=5,num_beams = 5)[0]['generated_text'])\n",
    "print(generator('for i in' , max_length=5 , do_sample=True,temperature = 0.7)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next():\n",
    "    from transformers import OpenAIGPTTokenizer,OpenAIGPTLMHeadModel,\\\n",
    "    TextDataset,TrainingArguments,Trainer,pipeline,DataCollatorForLanguageModeling\n",
    "    import re \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    \n",
    "    \n",
    "    tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "    model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
    "    generator = pipeline('text-generation', tokenizer='openai-gpt', model='gpt_model') \n",
    "    while(True):\n",
    "        text = input('Enter the text: ')\n",
    "        length= len(tokenizer.encode(text, return_tensors='pt')[0])\n",
    "        \n",
    "        max_length = length+1\n",
    "    \n",
    "        print('Next Word: ')\n",
    "        print(generator(text , max_length=max_length)[0]['generated_text'].split(' ')[-1])\n",
    "        print(generator(text , max_length=max_length , num_beams = 5)[0]['generated_text'].split(' ')[-1])\n",
    "        print(generator(text , max_length=max_length , do_sample=True,temperature = 0.7)[0]['generated_text'].split(' ')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1149091... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">gpt_model</strong>: <a href=\"https://wandb.ai/rohangoli/huggingface/runs/f1day7bx\" target=\"_blank\">https://wandb.ai/rohangoli/huggingface/runs/f1day7bx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211127_130018-f1day7bx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1274"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 27 15:11:54 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    39W / 300W |     23MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  Off  | 00000000:1C:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    40W / 300W |     23MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  Off  | 00000000:1D:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    38W / 300W |     23MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  Off  | 00000000:1E:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    38W / 300W |     23MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4628      G   /usr/libexec/Xorg                  22MiB |\n",
      "|    1   N/A  N/A      4628      G   /usr/libexec/Xorg                  22MiB |\n",
      "|    2   N/A  N/A      4628      G   /usr/libexec/Xorg                  22MiB |\n",
      "|    3   N/A  N/A      4628      G   /usr/libexec/Xorg                  22MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi --gpu-reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorflowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
