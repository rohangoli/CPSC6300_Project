{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Dropout, LSTMCell, RNN, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import pickle\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import requests\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "import argparse\n",
    "from tokenize import tokenize, untokenize, COMMENT, STRING, NEWLINE, ENCODING, ENDMARKER, NL, INDENT, NUMBER\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"Project_CodeNet_Python800.tar.gz\"\n",
    "data_url = f\"https://dax-cdn.cdn.appdomain.cloud/dax-project-codenet/1.0.0/{file_name}\"\n",
    "\n",
    "# Download tar archive to local disk\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(requests.get(data_url).content)\n",
    "    \n",
    "# Extract contents of archive to local disk\n",
    "if os.path.exists(\"data\"):\n",
    "    shutil.rmtree(\"data\")    \n",
    "with tarfile.open(file_name) as tfile:\n",
    "    tfile.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = glob.glob(os.path.join(os.getcwd(),\"Project_CodeNet_Python800/**/*.*\"))\n",
    "# len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lits = json.load(open(\"literals.json\"))\n",
    "\n",
    "def process_string(token, special_chars={\" \": \"U+0020\", \",\": \"U+002C\"}):\n",
    "    str_quote_options = [\"'''\", '\"\"\"', \"'\", '\"']\n",
    "    start_quote = \"\"\n",
    "    end_quote = \"\"\n",
    "    qualifier_regex = r\"^[a-z]+\"\n",
    "    qualifier_match = re.search(qualifier_regex, token)\n",
    "    # string qualifiers like 'r' for regex, 'f' for formatted string, 'b' for bytes, 'u' for unicode, etc (or combination of them)\n",
    "    qualifier = \"\" if not qualifier_match else qualifier_match[0]\n",
    "    # token string without qualifiers\n",
    "    token_string = re.sub(qualifier_regex, \"\", token)\n",
    "    # string literal without quotes\n",
    "    str_lit = token_string\n",
    "    for q in str_quote_options:\n",
    "        if token_string.startswith(q):\n",
    "            start_quote = q\n",
    "            str_lit = str_lit[len(q) :]\n",
    "            if token_string.endswith(q):\n",
    "                end_quote = q\n",
    "                str_lit = str_lit[: -len(q)]\n",
    "            break\n",
    "    # if start_quote in str_quote_options[:2]:\n",
    "    #     return \"\"\n",
    "    for sc in special_chars:\n",
    "        str_lit = str_lit.replace(sc, special_chars[sc])\n",
    "    return (\n",
    "        f\"{qualifier}{start_quote}<STR_LIT:{str_lit}>{end_quote}\"\n",
    "        if str_lit in lits['str']\n",
    "        else f\"{qualifier}{start_quote}<STR_LIT>{end_quote}\"\n",
    "    )\n",
    "\n",
    "def py_tokenize():\n",
    "    file_paths = glob.glob(os.path.join(os.getcwd(),\"Project_CodeNet_Python800/**/*.*\"))\n",
    "    wf = open(os.path.join(os.getcwd(), f\"full_corpus.txt\"), 'w')\n",
    "    local_corpus = []\n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            code = open(path).read()\n",
    "            token_gen = tokenize(BytesIO(bytes(code, \"utf8\")).readline)\n",
    "            out_tokens = []\n",
    "            prev_eol = False\n",
    "            for toknum, tokval, _, _, _ in token_gen:\n",
    "                tokval = \" \".join(tokval.split())\n",
    "                if toknum == STRING:\n",
    "                    add_token = process_string(tokval)\n",
    "                    out_tokens.append(add_token)\n",
    "                    prev_eol = False\n",
    "                elif toknum == NUMBER:\n",
    "                    if tokval in lits['num']:\n",
    "                        out_tokens.append(f\"<NUM_LIT:{tokval}>\")\n",
    "                    else:\n",
    "                        out_tokens.append(f\"<NUM_LIT>\")\n",
    "                    prev_eol = False\n",
    "                elif toknum in [NEWLINE, NL]:\n",
    "                    if not prev_eol:\n",
    "                        out_tokens.append(\"<EOL>\")\n",
    "                        prev_eol = True\n",
    "                elif toknum in [COMMENT, INDENT, ENCODING, ENDMARKER] or len(tokval) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    out_tokens.append(tokval)\n",
    "                    prev_eol = False\n",
    "            if out_tokens[0] == \"<EOL>\":\n",
    "                out_tokens = out_tokens[1:]\n",
    "            if out_tokens[-1] == \"<EOL>\":\n",
    "                out_tokens = out_tokens[:-1]\n",
    "        except Exception:\n",
    "            out_tokens = []\n",
    "#         local_corpus.extend((\" \".join(out_tokens)).split('<EOL>'))\n",
    "#         out_tokens = [\"<s>\"] + out_tokens + [\"</s>\"]\n",
    "        out = \" \".join(out_tokens)\n",
    "        local_corpus.append(out)\n",
    "        wf.write(out+\"\\n\")\n",
    "    print(f\"Full Corpus is done\")\n",
    "    wf.close()\n",
    "    return local_corpus\n",
    "\n",
    "def read_corpus():\n",
    "    corpus = py_tokenize()\n",
    "    full_corpus = ''.join(corpus)\n",
    "    corpus_new = []\n",
    "    for code in corpus:\n",
    "        corpus_new.extend(code.split('<EOL>'))\n",
    "        \n",
    "    return pd.DataFrame(corpus_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('full_corpus.txt', encoding='utf8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e = enumerate <EOL> n , * a = map ( int , open ( <NUM_LIT:0> ) . read ( ) . split ( ) ) <EOL> d = [ <NUM_LIT:0> ] <EOL> for j , ( a , i ) in e ( sorted ( ( a , i ) for i , a in e ( a ) ) [ : : - <NUM_LIT:1> ] ) : d = [ d [ <NUM_LIT:0> ] + a * abs ( n - j - i - <NUM_LIT:1> ) ] + [ max ( d [ k ] + a * abs ( n - j + k - i - <NUM_LIT:1> ) , d [ k - <NUM_LIT:1> ] + a * abs ( i - k + <NUM_LIT:1> ) ) for k in range ( <NUM_LIT:1> , j + <NUM_LIT:1> ) ] + [ d [ j ] + a * abs ( i - j ) ] <EOL> print ( max ( d ) )\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_corpus = []\n",
    "# for code in corpus:\n",
    "#     text_corpus.extend(code.split(' <EOL> '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_corpus[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent = corpus[0:int(0.8*len(corpus))]\n",
    "test_sent = corpus[int(0.8*len(corpus)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N = int ( input ( ) ) <EOL> count = <NUM_LIT:0> <EOL> for i in range ( <NUM_LIT:1> , N + <NUM_LIT:1> ) : <EOL> a = str ( i ) <EOL> b = len ( a ) <EOL> if b % <NUM_LIT:2> != <NUM_LIT:0> : <EOL> count += <NUM_LIT:1> <EOL> else : <EOL> count += <NUM_LIT:0> <EOL> print ( count )\\n',\n",
       " 'import math <EOL> N = int ( input ( ) ) <EOL> print ( sum ( [ int ( math . log10 ( x ) ) % <NUM_LIT:2> == <NUM_LIT:0> for x in range ( <NUM_LIT:1> , N + <NUM_LIT:1> ) ] ) )\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"train_sent_byCode.txt\", \"w\") as fp:   #Pickling\n",
    "    fp.write('\\n'.join(train_sent))\n",
    "with open(\"test_sent_byCode.txt\", \"w\") as fp:   #Pickling\n",
    "    fp.write('\\n'.join(test_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print (torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer,OpenAIGPTLMHeadModel,TextDataset,TrainingArguments,Trainer,pipeline,DataCollatorForLanguageModeling, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't convert <tokenizers.trainers.BpeTrainer object at 0x149c00211d90> to Sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-3e7a061126ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# paths[1:5]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBPE_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# train the tokenizer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbpe_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# # saving the tokenized data in our specified folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/clemson/cpsc-6300/Project/Nov27-GPT/custom_tokenise.py\u001b[0m in \u001b[0;36mbpe_train\u001b[0;34m(self, paths)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;34m\"<mask>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         ])\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't convert <tokenizers.trainers.BpeTrainer object at 0x149c00211d90> to Sequence"
     ]
    }
   ],
   "source": [
    "# tokenizer = OpenAIGPTTokenizer.from_pretrained(\"congcongwang/gpt2_medium_fine_tuned_coder\")\n",
    "# tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "from custom_tokenise import BPE_token\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "paths = glob.glob(os.path.join(os.getcwd(),\"Project_CodeNet_Python800/**/*.py\"))\n",
    "# paths[1:5]\n",
    "tokenizer = BPE_token()# train the tokenizer model\n",
    "tokenizer.bpe_train(paths)\n",
    "\n",
    "# # saving the tokenized data in our specified folder \n",
    "# save_path = 'tokenized_data'\n",
    "# tokenizer.save_tokenizer(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OpenAIGPTLMHeadModel were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0,1,2,3], dim=0)\n",
    "if use_cuda:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 40478, max sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "print('vocabulary size: %d, max sequence length: %d' % (tokenizer.vocab_size, tokenizer.model_max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  243,   303,   496,    16,  1209,   746,   295,     5,  1156,   290,\n",
      "           247,   240,   269,   246,   303,  5126,   276,  1786,   240,  1189,\n",
      "           276,   295, 16324,   279,  2321,   271,   286,   290,   275,   239,\n",
      "          1456,   276,   275,   239,  4620,   276,   275,   275,   295,     5,\n",
      "          1156,   290,   248,   303,   293,   295, 16324,   279,  2321,   271,\n",
      "           286,   290,   294,   295,     5,  1156,   290,   562,   266,   240,\n",
      "           276,   246,   240,   249,   275,   500,   243,   276,  9840,   276,\n",
      "           276,   246,   240,   249,   275,   562,   249,   240,   246,   500,\n",
      "           243,   276,   246,   275,   275,   293,   271,   271,   260,   295,\n",
      "         16324,   279,  2321,   271,   277,   290,   294,   275,   271,   248,\n",
      "           303,   293,   248,   293,   295, 16324,   279,  2321,   271,   286,\n",
      "           290,   294,   306,   246,   269, 12491,   276,   247,   260,   266,\n",
      "           260,   249,   260,   295, 16324,   279,  2321,   271,   277,   290,\n",
      "           275,   294,   306,   293,  2942,   276,   248,   293,   265,   294,\n",
      "           306,   246,   269, 12491,   276,   247,   260,   266,   306,   265,\n",
      "           260,   249,   260,   295, 16324,   279,  2321,   271,   277,   290,\n",
      "           275,   240,   248,   293,   265,   260,   295, 16324,   279,  2321,\n",
      "           271,   277,   290,   294,   306,   246,   269, 12491,   276,   249,\n",
      "           260,   265,   306,   295, 16324,   279,  2321,   271,   277,   290,\n",
      "           275,   275,   562,   265,   500,  5855,   276,   295, 16324,   279,\n",
      "          2321,   271,   277,   290,   240,   266,   306,   295, 16324,   279,\n",
      "          2321,   271,   277,   290,   275,   294,   306,   293,   248,   293,\n",
      "           266,   294,   306,   246,   269, 12491,   276,   249,   260,   266,\n",
      "           275,   294,   295,     5,  1156,   290,  8230,   276,  2942,   276,\n",
      "           248,   275,   275]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(train_sent[0], return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e</w>',\n",
       " '=</w>',\n",
       " 'en',\n",
       " 'u',\n",
       " 'mer',\n",
       " 'ate</w>',\n",
       " '<</w>',\n",
       " 'e',\n",
       " 'ol</w>',\n",
       " '></w>',\n",
       " 'n</w>',\n",
       " ',</w>',\n",
       " '*</w>',\n",
       " 'a</w>',\n",
       " '=</w>',\n",
       " 'map</w>',\n",
       " '(</w>',\n",
       " 'int</w>',\n",
       " ',</w>',\n",
       " 'open</w>',\n",
       " '(</w>',\n",
       " '<</w>',\n",
       " 'num</w>',\n",
       " '_</w>',\n",
       " 'lit</w>',\n",
       " ':</w>',\n",
       " '0</w>',\n",
       " '></w>',\n",
       " ')</w>',\n",
       " '.</w>',\n",
       " 'read</w>',\n",
       " '(</w>',\n",
       " ')</w>',\n",
       " '.</w>',\n",
       " 'split</w>',\n",
       " '(</w>',\n",
       " ')</w>',\n",
       " ')</w>',\n",
       " '<</w>',\n",
       " 'e',\n",
       " 'ol</w>',\n",
       " '></w>',\n",
       " 'd</w>',\n",
       " '=</w>',\n",
       " '[</w>',\n",
       " '<</w>',\n",
       " 'num</w>',\n",
       " '_</w>',\n",
       " 'lit</w>',\n",
       " ':</w>',\n",
       " '0</w>',\n",
       " '></w>',\n",
       " ']</w>',\n",
       " '<</w>',\n",
       " 'e',\n",
       " 'ol</w>',\n",
       " '></w>',\n",
       " 'for</w>',\n",
       " 'j</w>',\n",
       " ',</w>',\n",
       " '(</w>',\n",
       " 'a</w>',\n",
       " ',</w>',\n",
       " 'i</w>',\n",
       " ')</w>',\n",
       " 'in</w>',\n",
       " 'e</w>',\n",
       " '(</w>',\n",
       " 'sorted</w>',\n",
       " '(</w>',\n",
       " '(</w>',\n",
       " 'a</w>',\n",
       " ',</w>',\n",
       " 'i</w>',\n",
       " ')</w>',\n",
       " 'for</w>',\n",
       " 'i</w>',\n",
       " ',</w>',\n",
       " 'a</w>',\n",
       " 'in</w>',\n",
       " 'e</w>',\n",
       " '(</w>',\n",
       " 'a</w>',\n",
       " ')</w>',\n",
       " ')</w>',\n",
       " '[</w>',\n",
       " ':</w>',\n",
       " ':</w>',\n",
       " '-</w>',\n",
       " '<</w>',\n",
       " 'num</w>',\n",
       " '_</w>',\n",
       " 'lit</w>',\n",
       " ':</w>',\n",
       " '1</w>',\n",
       " '></w>',\n",
       " ']</w>',\n",
       " ')</w>',\n",
       " ':</w>',\n",
       " 'd</w>',\n",
       " '=</w>',\n",
       " '[</w>',\n",
       " 'd</w>',\n",
       " '[</w>',\n",
       " '<</w>',\n",
       " 'num</w>',\n",
       " '_</w>',\n",
       " 'lit</w>',\n",
       " ':</w>',\n",
       " '0</w>',\n",
       " '></w>',\n",
       " ']</w>',\n",
       " '+</w>',\n",
       " 'a</w>',\n",
       " '*</w>',\n",
       " 'abs</w>',\n",
       " '(</w>',\n",
       " 'n</w>',\n",
       " '-</w>',\n",
       " 'j</w>',\n",
       " '-</w>',\n",
       " 'i</w>',\n",
       " '-</w>',\n",
       " '<</w>',\n",
       " 'num</w>',\n",
       " '_</w>',\n",
       " 'lit</w>',\n",
       " ':</w>',\n",
       " '1</w>',\n",
       " '></w>',\n",
       " ')</w>',\n",
       " ']</w>',\n",
       " '+</w>',\n",
       " '[</w>',\n",
       " 'max</w>',\n",
       " '(</w>',\n",
       " 'd</w>',\n",
       " '[</w>',\n",
       " 'k</w>',\n",
       " ']</w>',\n",
       " '+</w>',\n",
       " 'a</w>',\n",
       " '*</w>',\n",
       " 'abs</w>',\n",
       " '(</w>',\n",
       " 'n</w>',\n",
       " '-</w>',\n",
       " 'j</w>',\n",
       " '+</w>',\n",
       " 'k</w>',\n",
       " '-</w>',\n",
       " 'i</w>',\n",
       " '-</w>',\n",
       " '<</w>',\n",
       " 'num</w>',\n",
       " '_</w>',\n",
       " 'lit</w>',\n",
       " ':</w>',\n",
       " '1</w>',\n",
       " '></w>',\n",
       " ')</w>',\n",
       " ',</w>',\n",
       " 'd</w>',\n",
       " '[</w>',\n",
       " 'k</w>',\n",
       " '-</w>',\n",
       " '<</w>',\n",
       " 'num</w>',\n",
       " '_</w>',\n",
       " 'lit</w>',\n",
       " ':</w>',\n",
       " '1</w>',\n",
       " '></w>',\n",
       " ']</w>',\n",
       " '+</w>',\n",
       " 'a</w>',\n",
       " '*</w>',\n",
       " 'abs</w>',\n",
       " '(</w>',\n",
       " 'i</w>',\n",
       " '-</w>',\n",
       " 'k</w>',\n",
       " '+</w>',\n",
       " '<</w>',\n",
       " 'num</w>',\n",
       " '_</w>',\n",
       " 'lit</w>',\n",
       " ':</w>',\n",
       " '1</w>',\n",
       " '></w>',\n",
       " ')</w>',\n",
       " ')</w>',\n",
       " 'for</w>',\n",
       " 'k</w>',\n",
       " 'in</w>',\n",
       " 'range</w>',\n",
       " '(</w>',\n",
       " '<</w>',\n",
       " 'num</w>',\n",
       " '_</w>',\n",
       " 'lit</w>',\n",
       " ':</w>',\n",
       " '1</w>',\n",
       " '></w>',\n",
       " ',</w>',\n",
       " 'j</w>',\n",
       " '+</w>',\n",
       " '<</w>',\n",
       " 'num</w>',\n",
       " '_</w>',\n",
       " 'lit</w>',\n",
       " ':</w>',\n",
       " '1</w>',\n",
       " '></w>',\n",
       " ')</w>',\n",
       " ']</w>',\n",
       " '+</w>',\n",
       " '[</w>',\n",
       " 'd</w>',\n",
       " '[</w>',\n",
       " 'j</w>',\n",
       " ']</w>',\n",
       " '+</w>',\n",
       " 'a</w>',\n",
       " '*</w>',\n",
       " 'abs</w>',\n",
       " '(</w>',\n",
       " 'i</w>',\n",
       " '-</w>',\n",
       " 'j</w>',\n",
       " ')</w>',\n",
       " ']</w>',\n",
       " '<</w>',\n",
       " 'e',\n",
       " 'ol</w>',\n",
       " '></w>',\n",
       " 'print</w>',\n",
       " '(</w>',\n",
       " 'max</w>',\n",
       " '(</w>',\n",
       " 'd</w>',\n",
       " ')</w>',\n",
       " ')</w>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([  243,   303,   496,    16,  1209,   746,   295,     5,  1156,   290,\n",
    "           247,   240,   269,   246,   303,  5126,   276,  1786,   240,  1189,\n",
    "           276,   295, 16324,   279,  2321,   271,   286,   290,   275,   239,\n",
    "          1456,   276,   275,   239,  4620,   276,   275,   275,   295,     5,\n",
    "          1156,   290,   248,   303,   293,   295, 16324,   279,  2321,   271,\n",
    "           286,   290,   294,   295,     5,  1156,   290,   562,   266,   240,\n",
    "           276,   246,   240,   249,   275,   500,   243,   276,  9840,   276,\n",
    "           276,   246,   240,   249,   275,   562,   249,   240,   246,   500,\n",
    "           243,   276,   246,   275,   275,   293,   271,   271,   260,   295,\n",
    "         16324,   279,  2321,   271,   277,   290,   294,   275,   271,   248,\n",
    "           303,   293,   248,   293,   295, 16324,   279,  2321,   271,   286,\n",
    "           290,   294,   306,   246,   269, 12491,   276,   247,   260,   266,\n",
    "           260,   249,   260,   295, 16324,   279,  2321,   271,   277,   290,\n",
    "           275,   294,   306,   293,  2942,   276,   248,   293,   265,   294,\n",
    "           306,   246,   269, 12491,   276,   247,   260,   266,   306,   265,\n",
    "           260,   249,   260,   295, 16324,   279,  2321,   271,   277,   290,\n",
    "           275,   240,   248,   293,   265,   260,   295, 16324,   279,  2321,\n",
    "           271,   277,   290,   294,   306,   246,   269, 12491,   276,   249,\n",
    "           260,   265,   306,   295, 16324,   279,  2321,   271,   277,   290,\n",
    "           275,   275,   562,   265,   500,  5855,   276,   295, 16324,   279,\n",
    "          2321,   271,   277,   290,   240,   266,   306,   295, 16324,   279,\n",
    "          2321,   271,   277,   290,   275,   294,   306,   293,   248,   293,\n",
    "           266,   294,   306,   246,   269, 12491,   276,   249,   260,   266,\n",
    "           275,   294,   295,     5,  1156,   290,  8230,   276,  2942,   276,\n",
    "           248,   275,   275])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/software/venv/tf1_gpu/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='train_sent.txt',\n",
    "    overwrite_cache=True,\n",
    "    block_size=19)\n",
    "\n",
    "test_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='test_sent.txt',\n",
    "    overwrite_cache=True,\n",
    "    block_size=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = 'gpt_model', \n",
    "    overwrite_output_dir = True, \n",
    "    per_device_train_batch_size = 64, \n",
    "    per_device_eval_batch_size = 64, \n",
    "    learning_rate = 5e-4, \n",
    "    num_train_epochs = 3,\n",
    ")\n",
    "# Initializing the trainer class object that will do the training\n",
    "# here the data collator will generate the batch of size 64 of train and test data\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2425650\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 28428\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrohangoli\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/rohangoli/huggingface/runs/g44pr3un\" target=\"_blank\">gpt_model</a></strong> to <a href=\"https://wandb.ai/rohangoli/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1412' max='28428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1412/28428 09:05 < 2:54:06, 2.59 it/s, Epoch 0.15/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.448500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.301200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to gpt_model/checkpoint-500\n",
      "Configuration saved in gpt_model/checkpoint-500/config.json\n",
      "Model weights saved in gpt_model/checkpoint-500/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to gpt_model/checkpoint-1000\n",
      "Configuration saved in gpt_model/checkpoint-1000/config.json\n",
      "Model weights saved in gpt_model/checkpoint-1000/pytorch_model.bin\n",
      "/home/rgoli/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "# Training the model for 3 epochs\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 1149091... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">gpt_model</strong>: <a href=\"https://wandb.ai/rohangoli/huggingface/runs/f1day7bx\" target=\"_blank\">https://wandb.ai/rohangoli/huggingface/runs/f1day7bx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211127_130018-f1day7bx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1274"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov 27 12:55:14 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 470.42.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:02:00.0 Off |                    0 |\n",
      "| N/A   18C    P0    28W / 250W |  12159MiB / 12198MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4126      G   /usr/libexec/Xorg                  22MiB |\n",
      "|    0   N/A  N/A   1147633      C   .../venv/tf1_gpu/bin/python3    12135MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi --gpu-reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorflowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
