{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Currently Loaded Modules:\n",
      "  1) anaconda3/5.1.0-gcc/8.3.1     4) cudnn/8.0.0.180-11.0-linux-x64-gcc/7.5.0\n",
      "  2) anaconda3/2019.10-gcc/8.3.1   5) openjdk/1.8.0_222-b10-gcc/8.3.1\n",
      "  3) cuda/11.0.3-gcc/7.5.0         6) hadoop/3.2.1-gcc/8.3.1\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "!module list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Flatten, TimeDistributed, Dropout, LSTMCell, RNN, Bidirectional, Concatenate, Layer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import requests\n",
    "import tarfile\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "# tf.config.experimental.set_memory_growth(physical_devices[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  train\n"
     ]
    }
   ],
   "source": [
    "file_name = \"Project_CodeNet_MLM.tar.gz\"\n",
    "data_url = f\"https://dax-cdn.cdn.appdomain.cloud/dax-project-codenet/1.0.0/{file_name}\"\n",
    "\n",
    "# Download tar archive to local disk\n",
    "with open(file_name, \"wb\") as f:\n",
    "    f.write(requests.get(data_url).content)\n",
    "    \n",
    "# Extract contents of archive to local disk\n",
    "if os.path.exists(\"tokens\"):\n",
    "    shutil.rmtree(\"tokens\")    \n",
    "with tarfile.open(file_name) as tfile:\n",
    "    tfile.extractall()\n",
    "    \n",
    "!ls tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  tokens\n",
      "49995  # include < id . id > int main ( void ) { int ...\n",
      "49996  # include < id . id > # include < id . id > # ...\n",
      "49997  # include < id . id > # include < id . id > in...\n",
      "49998  # include < id . id > int main ( void ) { int ...\n",
      "49999  # include < id . id > int main ( void ) { int ...\n"
     ]
    }
   ],
   "source": [
    "# Read all files and return content as list of lines.\n",
    "def get_text_list_from_files(files):\n",
    "    text_list = []\n",
    "    for name in files:\n",
    "        with open(name) as f:\n",
    "            for line in f:\n",
    "                text_list.append(line)\n",
    "    return text_list\n",
    "\n",
    "# Compose the full path names to the token files.\n",
    "# Creates and returns a dataframe with single key \"tokens\".\n",
    "def get_data_from_text_files(folder_name):\n",
    "    files = glob.glob(folder_name + '/*.toks')\n",
    "    texts = get_text_list_from_files(files)\n",
    "    df = pd.DataFrame({'tokens': texts})\n",
    "    df = df.sample(len(df)).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "train_data = get_data_from_text_files('tokens/train')\n",
    "print(train_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td># include &lt; id . id &gt; int main ( ) { int id , id , id , id , id , id ; scanf ( string , operator id ) ; scanf ( string , operator id ) ; scanf ( string , operator id ) ; scanf ( string , operator id ) ; scanf ( string , operator id ) ; if ( id &lt; number ) id = number ; if ( id &lt; number ) id = number ; if ( id &lt; number ) id = number ; if ( id &lt; number ) id = number ; if ( id &lt; number ) id = number ; id = ( id operator id operator id operator id operator id ) operator number ; printf ( string , id ) ; return 0 ; }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td># include &lt; id . id &gt; int main ( void ) { int id ; int id ; int id ; for ( id = 1 ; ; id operator ) { scanf ( string , operator id ) ; if ( id operator 0 ) { printf ( string , id , id ) ; } if ( id operator 0 ) { break ; } } return 0 ; }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td># include &lt; id . id &gt; # include &lt; id . id &gt; int main ( ) { int id ; double id , id , id , id , id , id , id , id [ number ] = { } ; while ( 1 ) { double id [ number ] = { } ; id = 0 ; id = 0 ; scanf ( string , operator id ) ; if ( id operator 0 ) break ; for ( id = 0 ; id &lt; id ; id operator ) { scanf ( string , operator id [ id ] ) ; id = id operator id [ id ] ; } id = id operator id ; for ( id = 0 ; id &lt; id ; id operator ) { id = id [ id ] operator id ; if ( id &lt; 0 ) id = id operator ( operator 1 ) ; id = pow ( id , number ) ; id = id operator id ; } id = id operator id ; id = pow ( id , number ) ; printf ( string , id ) ; } return 0 ; }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td># include &lt; id . id &gt; # include &lt; string . id &gt; int main ( ) { int id , id ; scanf ( string , operator id ) ; char id [ number ] ; scanf ( string , id ) ; for ( id = 0 ; id &lt; strlen ( id ) ; id operator ) { int id = id [ id ] operator id ; if ( id operator number ) { id [ id ] = id ; } else if ( id &gt; number ) { id [ id ] = id operator number ; } } printf ( string , id ) ; return 0 ; }</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td># include &lt; id . id &gt; int main ( ) { int id , id [ number ] [ number ] , id , id , id , id , id , id , id , id , id ; int id [ ] = { operator 1 , 0 , 1 , 0 } ; int id [ ] = { 0 , operator 1 , 0 , 1 } ; while ( scanf ( string , operator id ) , id ) { id = id = id = id = id [ 0 ] [ 0 ] = id [ 0 ] [ 1 ] = 0 ; for ( id = 1 ; id &lt; id ; id operator ) { scanf ( string , operator id , operator id ) ; id [ id ] [ 0 ] = id [ id ] [ 0 ] operator id [ id ] ; id [ id ] [ 1 ] = id [ id ] [ 1 ] operator id [ id ] ; if ( id [ id ] [ 0 ] &lt; id ) id = id [ id ] [ 0 ] ; if ( id [ id ] [ 0 ] &gt; id ) id = id [ id ] [ 0 ] ; if ( id [ id ] [ 1 ] &gt; id ) id = id [ id ] [ 1 ] ; if ( id [ id ] [ 1 ] &lt; id ) id = id [ id ] [ 1 ] ; } printf ( string , id operator id operator 1 , id operator id operator 1 ) ; } return 0 ; }</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                tokens\n",
       "0                                                                                                                                                                                                                                                                                                # include < id . id > int main ( ) { int id , id , id , id , id , id ; scanf ( string , operator id ) ; scanf ( string , operator id ) ; scanf ( string , operator id ) ; scanf ( string , operator id ) ; scanf ( string , operator id ) ; if ( id < number ) id = number ; if ( id < number ) id = number ; if ( id < number ) id = number ; if ( id < number ) id = number ; if ( id < number ) id = number ; id = ( id operator id operator id operator id operator id ) operator number ; printf ( string , id ) ; return 0 ; } \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       # include < id . id > int main ( void ) { int id ; int id ; int id ; for ( id = 1 ; ; id operator ) { scanf ( string , operator id ) ; if ( id operator 0 ) { printf ( string , id , id ) ; } if ( id operator 0 ) { break ; } } return 0 ; } \n",
       "2                                                                                                                                                              # include < id . id > # include < id . id > int main ( ) { int id ; double id , id , id , id , id , id , id , id [ number ] = { } ; while ( 1 ) { double id [ number ] = { } ; id = 0 ; id = 0 ; scanf ( string , operator id ) ; if ( id operator 0 ) break ; for ( id = 0 ; id < id ; id operator ) { scanf ( string , operator id [ id ] ) ; id = id operator id [ id ] ; } id = id operator id ; for ( id = 0 ; id < id ; id operator ) { id = id [ id ] operator id ; if ( id < 0 ) id = id operator ( operator 1 ) ; id = pow ( id , number ) ; id = id operator id ; } id = id operator id ; id = pow ( id , number ) ; printf ( string , id ) ; } return 0 ; } \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                 # include < id . id > # include < string . id > int main ( ) { int id , id ; scanf ( string , operator id ) ; char id [ number ] ; scanf ( string , id ) ; for ( id = 0 ; id < strlen ( id ) ; id operator ) { int id = id [ id ] operator id ; if ( id operator number ) { id [ id ] = id ; } else if ( id > number ) { id [ id ] = id operator number ; } } printf ( string , id ) ; return 0 ; } \n",
       "4  # include < id . id > int main ( ) { int id , id [ number ] [ number ] , id , id , id , id , id , id , id , id , id ; int id [ ] = { operator 1 , 0 , 1 , 0 } ; int id [ ] = { 0 , operator 1 , 0 , 1 } ; while ( scanf ( string , operator id ) , id ) { id = id = id = id = id [ 0 ] [ 0 ] = id [ 0 ] [ 1 ] = 0 ; for ( id = 1 ; id < id ; id operator ) { scanf ( string , operator id , operator id ) ; id [ id ] [ 0 ] = id [ id ] [ 0 ] operator id [ id ] ; id [ id ] [ 1 ] = id [ id ] [ 1 ] operator id [ id ] ; if ( id [ id ] [ 0 ] < id ) id = id [ id ] [ 0 ] ; if ( id [ id ] [ 0 ] > id ) id = id [ id ] [ 0 ] ; if ( id [ id ] [ 1 ] > id ) id = id [ id ] [ 1 ] ; if ( id [ id ] [ 1 ] < id ) id = id [ id ] [ 1 ] ; } printf ( string , id operator id operator 1 , id operator id operator 1 ) ; } return 0 ; } "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    MAX_LEN = 256               # length of each input sample in tokens\n",
    "    BATCH_SIZE = 32             # batch size\n",
    "    LR = 0.001                  # learning rate\n",
    "    VOCAB_SIZE = 256            # max. number of words in vocabulary\n",
    "    EMBED_DIM = 128             # word embedding vector size\n",
    "    NUM_HEAD = 8                # number of attention heads (BERT)\n",
    "    FF_DIM = 128                # feedforward dimension (BERT)\n",
    "    NUM_LAYERS = 1              # number of BERT module layers\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 203\n",
      "padding token vocab[0]: \"\"\n",
      "OOV token vocab[1]: \"[UNK]\"\n",
      "mask token vocab[202]: \"[mask]\"\n"
     ]
    }
   ],
   "source": [
    "# No special text filtering.\n",
    "def custom_standardization(input_data):\n",
    "    return input_data\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "# Create TextVectorization layer.\n",
    "def get_vectorize_layer(texts, vocab_size, max_seq):\n",
    "    vectorize_layer = TextVectorization(\n",
    "        max_tokens=vocab_size,\n",
    "        output_mode='int',\n",
    "        standardize=custom_standardization,\n",
    "        output_sequence_length=max_seq,\n",
    "    )\n",
    "    # Create vocabulary over all texts:\n",
    "    vectorize_layer.adapt(texts)\n",
    "    # Insert special mask token in vocabulary:\n",
    "    vocab = vectorize_layer.get_vocabulary()\n",
    "    vocab = vocab[2:len(vocab)-1] + ['[mask]']\n",
    "    vectorize_layer.set_vocabulary(vocab)\n",
    "    return vectorize_layer\n",
    "\n",
    "vectorize_layer = get_vectorize_layer(\n",
    "    train_data.tokens.values.tolist(),\n",
    "    config.VOCAB_SIZE,\n",
    "    config.MAX_LEN,\n",
    ")\n",
    "\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print('vocabulary size:', len(vocab))\n",
    "print('padding token vocab[0]: \"%s\"' % vocab[0])\n",
    "print('OOV token vocab[1]: \"%s\"' % vocab[1])\n",
    "print('mask token vocab[%d]: \"%s\"' % (len(vocab)-1, vocab[len(vocab)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: are done\n",
      "['arr = [ <NUM_LIT:0> ] * <NUM_LIT:100> ', ' while True : ', ' try : ', ' x , y , s = map ( int , input ( ) . split ( \"<STR_LIT:U+002C>\" ) ) ', ' if s == <NUM_LIT:3> : ', ' if x <= <NUM_LIT:7> : ', ' arr [ <NUM_LIT:10> * y + x + <NUM_LIT:2> ] += <NUM_LIT:1> ', ' if x >= <NUM_LIT:2> : ', ' arr [ <NUM_LIT:10> * y + x - <NUM_LIT:2> ] += <NUM_LIT:1> ', ' if y <= <NUM_LIT:7> : ', ' arr [ <NUM_LIT:10> * y + x + <NUM_LIT:20> ] += <NUM_LIT:1> ', ' if y >= <NUM_LIT:2> : ', ' arr [ <NUM_LIT:10> * y + x - <NUM_LIT:20> ] += <NUM_LIT:1> ', ' if s >= <NUM_LIT:2> : ', ' if x != <NUM_LIT:9> and y != <NUM_LIT:9> : ', ' arr [ <NUM_LIT:10> * y + x + <NUM_LIT:11> ] += <NUM_LIT:1> ', ' if x != <NUM_LIT:9> and y != <NUM_LIT:0> : ', ' arr [ <NUM_LIT:10> * y + x - <NUM_LIT:9> ] += <NUM_LIT:1> ', ' if x != <NUM_LIT:0> and y != <NUM_LIT:0> : ', ' arr [ <NUM_LIT:10> * y + x - <NUM_LIT:11> ] += <NUM_LIT:1> ']\n",
      "3815\n"
     ]
    }
   ],
   "source": [
    "corpus = py_tokenize(\"train\")\n",
    "corpus_new = []\n",
    "for code in corpus:\n",
    "    corpus_new.extend(code.split('<EOL>'))\n",
    "print(corpus_new[0:20])\n",
    "print(len(corpus_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arr', '=', '[', '<NUM_LIT:0>', ']', '*', '<NUM_LIT:100>', '<EOL>', 'while', 'True', ':', '<EOL>', 'try', ':', '<EOL>', 'x', ',', 'y', ',', 's', '=', 'map', '(', 'int', ',', 'input', '(', ')', '.', 'split', '(', '\"<STR_LIT:U+002C>\"', ')', ')', '<EOL>', 'if', 's', '==', '<NUM_LIT:3>', ':', '<EOL>', 'if', 'x', '<=', '<NUM_LIT:7>', ':', '<EOL>', 'arr', '[', '<NUM_LIT:10>', '*', 'y', '+', 'x', '+', '<NUM_LIT:2>', ']', '+=', '<NUM_LIT:1>', '<EOL>', 'if', 'x', '>=', '<NUM_LIT:2>', ':', '<EOL>', 'arr', '[', '<NUM_LIT:10>', '*', 'y', '+', 'x', '-', '<NUM_LIT:2>', ']', '+=', '<NUM_LIT:1>', '<EOL>', 'if', 'y', '<=', '<NUM_LIT:7>', ':', '<EOL>', 'arr', '[', '<NUM_LIT:10>', '*', 'y', '+', 'x', '+', '<NUM_LIT:20>', ']', '+=', '<NUM_LIT:1>', '<EOL>', 'if', 'y']\n"
     ]
    }
   ],
   "source": [
    "full_corpus_tokens = ''.join(corpus).split()\n",
    "print(full_corpus_tokens[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['arr', '=', '[', '<NUM_LIT:0>'], ['=', '[', '<NUM_LIT:0>', ']'], ['[', '<NUM_LIT:0>', ']', '*'], ['<NUM_LIT:0>', ']', '*', '<NUM_LIT:100>'], [']', '*', '<NUM_LIT:100>', '<EOL>'], ['*', '<NUM_LIT:100>', '<EOL>', 'while'], ['<NUM_LIT:100>', '<EOL>', 'while', 'True'], ['<EOL>', 'while', 'True', ':'], ['while', 'True', ':', '<EOL>'], ['True', ':', '<EOL>', 'try']]\n"
     ]
    }
   ],
   "source": [
    "train_len = 3+1\n",
    "text_sequences = []\n",
    "for i in range(train_len,len(full_corpus_tokens)):\n",
    "    seq = full_corpus_tokens[i-train_len:i]\n",
    "    text_sequences.append(seq)\n",
    "sequences = {}\n",
    "count = 1\n",
    "for i in range(len(full_corpus_tokens)):\n",
    "    if full_corpus_tokens[i] not in sequences:\n",
    "        sequences[full_corpus_tokens[i]] = count\n",
    "        count += 1\n",
    "print(text_sequences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences) \n",
    "\n",
    "#Collecting some information   \n",
    "vocabulary_size = len(tokenizer.word_counts)+1\n",
    "\n",
    "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
    "for i in range(len(sequences)):\n",
    "    n_sequences[i] = sequences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40546, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs = n_sequences[:,:-1]\n",
    "train_targets = n_sequences[:,-1]\n",
    "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)\n",
    "seq_len = train_inputs.shape[1]\n",
    "train_inputs.shape\n",
    "#print(train_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 3, 3)              2322      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 3, 50)             10800     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 774)               39474     \n",
      "=================================================================\n",
      "Total params: 75,346\n",
      "Trainable params: 75,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "1268/1268 [==============================] - 9s 5ms/step - loss: 4.6838 - accuracy: 0.0875\n",
      "Epoch 2/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 3.8811 - accuracy: 0.1582\n",
      "Epoch 3/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 3.1094 - accuracy: 0.2894\n",
      "Epoch 4/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 2.8118 - accuracy: 0.3442\n",
      "Epoch 5/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 2.6609 - accuracy: 0.3762\n",
      "Epoch 6/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 2.5458 - accuracy: 0.3879\n",
      "Epoch 7/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 2.4180 - accuracy: 0.4184\n",
      "Epoch 8/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 2.3120 - accuracy: 0.4337\n",
      "Epoch 9/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 2.2332 - accuracy: 0.4498\n",
      "Epoch 10/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 2.1394 - accuracy: 0.4712\n",
      "Epoch 11/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 2.0942 - accuracy: 0.4772\n",
      "Epoch 12/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 2.0505 - accuracy: 0.4793\n",
      "Epoch 13/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.9898 - accuracy: 0.4909\n",
      "Epoch 14/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.9404 - accuracy: 0.4995\n",
      "Epoch 15/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.9227 - accuracy: 0.5063\n",
      "Epoch 16/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.8647 - accuracy: 0.5144\n",
      "Epoch 17/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.8389 - accuracy: 0.5204\n",
      "Epoch 18/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.8132 - accuracy: 0.5217\n",
      "Epoch 19/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.7970 - accuracy: 0.5238\n",
      "Epoch 20/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.7601 - accuracy: 0.5336\n",
      "Epoch 21/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.7335 - accuracy: 0.5347\n",
      "Epoch 22/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.7337 - accuracy: 0.5352\n",
      "Epoch 23/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.7004 - accuracy: 0.5410\n",
      "Epoch 24/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.6682 - accuracy: 0.5488\n",
      "Epoch 25/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.6584 - accuracy: 0.5463\n",
      "Epoch 26/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.6449 - accuracy: 0.5530\n",
      "Epoch 27/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.6384 - accuracy: 0.5535\n",
      "Epoch 28/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.6034 - accuracy: 0.5576\n",
      "Epoch 29/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.5884 - accuracy: 0.5615\n",
      "Epoch 30/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.5673 - accuracy: 0.5701\n",
      "Epoch 31/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.5780 - accuracy: 0.5628\n",
      "Epoch 32/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.5487 - accuracy: 0.5699\n",
      "Epoch 33/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.5382 - accuracy: 0.5716\n",
      "Epoch 34/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.5249 - accuracy: 0.5773\n",
      "Epoch 35/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.5105 - accuracy: 0.5764\n",
      "Epoch 36/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.5072 - accuracy: 0.5760\n",
      "Epoch 37/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4950 - accuracy: 0.5800\n",
      "Epoch 38/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4776 - accuracy: 0.5829\n",
      "Epoch 39/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4902 - accuracy: 0.5797\n",
      "Epoch 40/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4651 - accuracy: 0.5834\n",
      "Epoch 41/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4658 - accuracy: 0.5842\n",
      "Epoch 42/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4394 - accuracy: 0.5898\n",
      "Epoch 43/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4297 - accuracy: 0.5938\n",
      "Epoch 44/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4210 - accuracy: 0.5984\n",
      "Epoch 45/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4237 - accuracy: 0.5895\n",
      "Epoch 46/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4159 - accuracy: 0.5913\n",
      "Epoch 47/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.4062 - accuracy: 0.5959\n",
      "Epoch 48/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.3903 - accuracy: 0.5991\n",
      "Epoch 49/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.3909 - accuracy: 0.6021\n",
      "Epoch 50/50\n",
      "1268/1268 [==============================] - 6s 5ms/step - loss: 1.3765 - accuracy: 0.6040\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Embedding\n",
    "#model = load_model(\"mymodel.h5\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, seq_len, input_length=seq_len))\n",
    "model.add(LSTM(50,return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(50,activation='relu'))\n",
    "model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(train_inputs,train_targets,epochs=50,verbose=1)\n",
    "model.save(\"mymodel_lstm.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('lstm_port.h5')\n",
    "model = load_model('lstm_port.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " if x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: ['+', 'in', '==']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " for i\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: ['in', ',', '(']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " for i,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: ['j', 'i', 'x']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " for i in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: ['range', 'xrange', 'sys']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " for i in range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: ['(', '-', '<num_lit:2>']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " import\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: [\"'<str_lit>'\", '[', 'input']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " import math\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: ['.', ',', ')']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " import math.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: [\"'<str_lit>'\", '[', 'input']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: ['.', ',', ':']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " import sys,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: [\"'<str_lit>'\", '[', 'input']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " return\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word suggestion: ['[', 'int', 'sys']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " stop\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "input_text = input().strip().lower()\n",
    "while(input_text != 'stop'):\n",
    "    encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "#     print(encoded_text, pad_encoded)\n",
    "    predictions = []\n",
    "    for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
    "        pred_word = tokenizer.index_word[i]\n",
    "        predictions.append(pred_word)\n",
    "    print(\"Next word suggestion:\",predictions)\n",
    "    input_text = input().strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorflowGPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
